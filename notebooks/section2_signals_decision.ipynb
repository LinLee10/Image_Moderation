{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Signals & Decision Engine\n",
    "\n",
    "**Goal**: NSFW scalar + OCR keywords + rule engine + end-to-end decision helper.\n",
    "\n",
    "**Scope**: Days 3-7 of MVP build. Combine hash matching with ML classifiers and policy rules.\n",
    "\n",
    "**Links**:\n",
    "- OpenNSFW2: https://github.com/bhky/opennsfw2\n",
    "- PaddleOCR: https://github.com/PaddlePaddle/PaddleOCR\n",
    "- TikTok Content Levels: https://support.tiktok.com/en/safety-hc/account-and-user-safety/content-levels-on-tiktok-posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet torch torchvision opennsfw2 paddleocr paddlepaddle pillow numpy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import opennsfw2\n",
    "import paddleocr\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"OpenNSFW2 version: {opennsfw2.__version__}\")\n",
    "print(f\"PaddleOCR version: {paddleocr.__version__}\")\n",
    "print(f\"Device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_nsfw_model = None\n",
    "\n",
    "def load_nsfw_model():\n",
    "    global _nsfw_model\n",
    "    if _nsfw_model is None:\n",
    "        _nsfw_model = opennsfw2.make_open_nsfw_model()\n",
    "        _nsfw_model.eval()\n",
    "        if torch.cuda.is_available():\n",
    "            _nsfw_model = _nsfw_model.cuda()\n",
    "    return _nsfw_model\n",
    "\n",
    "def score_nsfw(image_path):\n",
    "    try:\n",
    "        model = load_nsfw_model()\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        inputs = opennsfw2.preprocess_image(image, opennsfw2.Preprocessing.YAHOO)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            nsfw_probability = torch.sigmoid(outputs[0]).cpu().item()\n",
    "        return float(nsfw_probability)\n",
    "    except Exception as e:\n",
    "        print(f\"NSFW scoring failed for {image_path}: {e}\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ocr_model = None\n",
    "\n",
    "def load_ocr_model():\n",
    "    global _ocr_model\n",
    "    if _ocr_model is None:\n",
    "        _ocr_model = paddleocr.PaddleOCR(use_angle_cls=True, lang='en', show_log=False)\n",
    "    return _ocr_model\n",
    "\n",
    "def extract_text(image_path):\n",
    "    try:\n",
    "        ocr = load_ocr_model()\n",
    "        result = ocr.ocr(image_path, cls=True)\n",
    "        if result[0] is None:\n",
    "            return \"\"\n",
    "        \n",
    "        text_parts = []\n",
    "        for line in result[0]:\n",
    "            if len(line) >= 2 and line[1][1] > 0.5:\n",
    "                text_parts.append(line[1][0])\n",
    "        \n",
    "        return \" \".join(text_parts).strip()\n",
    "    except Exception as e:\n",
    "        print(f\"OCR failed for {image_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "HATE_KEYWORDS = [\"slur1\", \"slur2\", \"nazi\", \"terrorist\", \"hate\"]\n",
    "THREAT_KEYWORDS = [\"kill\", \"shoot\", \"bomb\", \"murder\", \"attack\"]\n",
    "SEXTORTION_KEYWORDS = [\"pay\", \"bitcoin\", \"leak\", \"expose\", \"money\", \"send\"]\n",
    "\n",
    "def flag_keywords(text):\n",
    "    if not text:\n",
    "        return {\"hate\": False, \"threat\": False, \"sextortion\": False}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    hate_flag = any(keyword in text_lower for keyword in HATE_KEYWORDS)\n",
    "    threat_flag = any(keyword in text_lower for keyword in THREAT_KEYWORDS)\n",
    "    sextortion_flag = any(keyword in text_lower for keyword in SEXTORTION_KEYWORDS)\n",
    "    \n",
    "    return {\n",
    "        \"hate\": hate_flag,\n",
    "        \"threat\": threat_flag,\n",
    "        \"sextortion\": sextortion_flag\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_DB = {\n",
    "    \"threat_feed:1234567890ab\": {\"hash\": \"1234567890abcdef\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"CSAM\"}},\n",
    "    \"threat_feed:fedcba098765\": {\"hash\": \"fedcba0987654321\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"TERROR\"}},\n",
    "    \"threat_feed:abcdef123456\": {\"hash\": \"abcdef1234567890\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"NCII\"}}\n",
    "}\n",
    "\n",
    "def compute_pdq(image_path):\n",
    "    try:\n",
    "        import pdqhash\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            hash_int, quality = pdqhash.compute(img)\n",
    "            hash_hex = f\"{hash_int:064x}\"\n",
    "            return hash_hex, quality\n",
    "    except:\n",
    "        import hashlib\n",
    "        with open(image_path, 'rb') as f:\n",
    "            content = f.read()\n",
    "        hash_hex = hashlib.md5(content).hexdigest().ljust(64, '0')\n",
    "        return hash_hex, 50\n",
    "\n",
    "def hamming_distance_hex(hex1, hex2):\n",
    "    return bin(int(hex1, 16) ^ int(hex2, 16)).count('1')\n",
    "\n",
    "def match_hash(query_hash, max_distance=30, topk=50):\n",
    "    matches = []\n",
    "    for media_id, data in HASH_DB.items():\n",
    "        distance = hamming_distance_hex(query_hash, data[\"hash\"])\n",
    "        if distance <= max_distance:\n",
    "            matches.append((media_id, distance, data.get(\"labels\", set())))\n",
    "    \n",
    "    matches.sort(key=lambda x: x[1])\n",
    "    return matches[:topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "POLICY = {\n    \"hash\": {\n        \"max_distance\": 30\n    },\n    \"nsfw\": {\n        \"threshold_adult_blur\": 0.95,\n        \"threshold_teen_block\": 0.85,\n        \"threshold_child_block\": 0.70\n    },\n    \"ocr\": {\n        \"enable\": True,\n        \"require_review_on_hate\": True,\n        \"require_review_on_threat\": True,\n        \"require_review_on_sextortion\": True,\n        \"confidence_threshold\": 0.7\n    },\n    \"age\": {\n        \"teen_mode\": False,\n        \"strict_mode\": False\n    },\n    \"surfaces\": {\n        \"feed\": {\"nsfw_multiplier\": 1.0, \"strictness\": \"normal\"},\n        \"dm\": {\"nsfw_multiplier\": 0.8, \"strictness\": \"relaxed\"},\n        \"avatar\": {\"nsfw_multiplier\": 1.2, \"strictness\": \"strict\"},\n        \"story\": {\"nsfw_multiplier\": 0.9, \"strictness\": \"normal\"}\n    }\n}\n\ndef decide(image_path, matches, nsfw_p, ocr_text, flags, teen_mode=False, surface=\"feed\"):\n    reasons = []\n    action = \"ALLOW\"\n    confidence = 0.0\n    \n    surface_config = POLICY[\"surfaces\"].get(surface, POLICY[\"surfaces\"][\"feed\"])\n    nsfw_adjusted = nsfw_p * surface_config[\"nsfw_multiplier\"]\n    \n    harmful_labels = {\"CSAM\", \"NCII\", \"TERROR\"}\n    for media_id, distance, labels in matches:\n        if distance <= POLICY[\"hash\"][\"max_distance\"] and labels & harmful_labels:\n            action = \"BLOCK\"\n            confidence = 1.0\n            reasons.append(f\"Hash match: {list(labels)} (distance: {distance})\")\n            break\n    \n    if action == \"ALLOW\":\n        if teen_mode:\n            if nsfw_adjusted >= POLICY[\"nsfw\"][\"threshold_teen_block\"]:\n                action = \"BLOCK\"\n                confidence = min(1.0, nsfw_adjusted)\n                reasons.append(f\"Teen NSFW block (score: {nsfw_adjusted:.3f})\")\n            elif nsfw_adjusted >= POLICY[\"nsfw\"][\"threshold_child_block\"]:\n                action = \"BLUR\"\n                confidence = min(1.0, nsfw_adjusted)\n                reasons.append(f\"Teen NSFW blur (score: {nsfw_adjusted:.3f})\")\n        else:\n            if nsfw_adjusted >= POLICY[\"nsfw\"][\"threshold_adult_blur\"]:\n                action = \"BLUR\"\n                confidence = min(1.0, nsfw_adjusted)\n                reasons.append(f\"Adult NSFW blur (score: {nsfw_adjusted:.3f})\")\n    \n    if action == \"ALLOW\" and POLICY[\"ocr\"][\"enable\"]:\n        review_flags = []\n        if flags.get(\"hate\") and POLICY[\"ocr\"][\"require_review_on_hate\"]:\n            review_flags.append(\"hate\")\n        if flags.get(\"threat\") and POLICY[\"ocr\"][\"require_review_on_threat\"]:\n            review_flags.append(\"threat\")\n        if flags.get(\"sextortion\") and POLICY[\"ocr\"][\"require_review_on_sextortion\"]:\n            review_flags.append(\"sextortion\")\n        \n        if review_flags:\n            action = \"REVIEW\"\n            confidence = 0.8\n            reasons.append(f\"OCR flags: {review_flags}\")\n    \n    if surface_config[\"strictness\"] == \"strict\" and action == \"ALLOW\" and nsfw_p > 0.5:\n        action = \"REVIEW\"\n        confidence = nsfw_p\n        reasons.append(f\"Strict surface review (score: {nsfw_p:.3f})\")\n    \n    return {\n        \"action\": action,\n        \"reasons\": reasons,\n        \"nsfw_p\": nsfw_p,\n        \"nsfw_adjusted\": nsfw_adjusted,\n        \"confidence\": confidence,\n        \"surface\": surface,\n        \"matches\": [(mid, dist) for mid, dist, _ in matches],\n        \"ocr_excerpt\": ocr_text[:100] + \"...\" if len(ocr_text) > 100 else ocr_text\n    }\n\ndef update_policy(updates):\n    for key, value in updates.items():\n        if key in POLICY:\n            if isinstance(POLICY[key], dict) and isinstance(value, dict):\n                POLICY[key].update(value)\n            else:\n                POLICY[key] = value\n    return POLICY.copy()\n\ndef get_policy_summary():\n    return {\n        \"hash_distance\": POLICY[\"hash\"][\"max_distance\"],\n        \"nsfw_thresholds\": POLICY[\"nsfw\"],\n        \"ocr_enabled\": POLICY[\"ocr\"][\"enable\"],\n        \"surfaces\": list(POLICY[\"surfaces\"].keys()),\n        \"current_mode\": \"teen\" if POLICY[\"age\"][\"teen_mode\"] else \"adult\"\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_decision(image_path, max_distance=None, teen_mode=None, surface=\"feed\"):\n    if max_distance is None:\n        max_distance = POLICY[\"hash\"][\"max_distance\"]\n    if teen_mode is None:\n        teen_mode = POLICY[\"age\"][\"teen_mode\"]\n    \n    start_time = time.time()\n    \n    hash_hex, quality = compute_pdq(image_path)\n    matches = match_hash(hash_hex, max_distance)\n    \n    nsfw_p = score_nsfw(image_path)\n    \n    ocr_text = extract_text(image_path)\n    flags = flag_keywords(ocr_text)\n    \n    decision = decide(image_path, matches, nsfw_p, ocr_text, flags, teen_mode, surface)\n    \n    end_time = time.time()\n    \n    result = {\n        \"image_path\": image_path,\n        \"hash_hex\": hash_hex,\n        \"hash_quality\": quality,\n        \"processing_time_ms\": int((end_time - start_time) * 1000),\n        \"teen_mode\": teen_mode,\n        \"ocr_flags\": flags,\n        **decision\n    }\n    \n    return result\n\ndef test_policy_scenarios():\n    print(\"\\\\nTesting policy scenarios across surfaces and age groups...\")\n    \n    surfaces = [\"feed\", \"dm\", \"avatar\", \"story\"]\n    age_modes = [False, True]  # Adult, Teen\n    \n    results = {}\n    \n    for surface in surfaces:\n        for teen_mode in age_modes:\n            age_label = \"teen\" if teen_mode else \"adult\"\n            key = f\"{surface}_{age_label}\"\n            \n            result = run_decision('test_images/nsfw.jpg', teen_mode=teen_mode, surface=surface)\n            results[key] = {\n                \"action\": result[\"action\"],\n                \"nsfw_p\": result[\"nsfw_p\"],\n                \"nsfw_adjusted\": result.get(\"nsfw_adjusted\", result[\"nsfw_p\"]),\n                \"confidence\": result.get(\"confidence\", 0.0)\n            }\n            \n    for key, result in results.items():\n        print(f\"  {key}: {result['action']} (NSFW: {result['nsfw_p']:.3f}â†’{result['nsfw_adjusted']:.3f}, conf: {result['confidence']:.3f})\")\n    \n    return results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_decide(image_paths):\n",
    "    results = []\n",
    "    for path in image_paths:\n",
    "        try:\n",
    "            result = run_decision(path)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                \"image_path\": path,\n",
    "                \"action\": \"ERROR\",\n",
    "                \"reasons\": [f\"Processing failed: {str(e)}\"],\n",
    "                \"nsfw_p\": 0.0,\n",
    "                \"processing_time_ms\": 0,\n",
    "                \"matches\": []\n",
    "            })\n",
    "    \n",
    "    print(f\"{'Image':<30} {'Action':<10} {'NSFW':<8} {'Min Dist':<8} {'Time(ms)':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for result in results:\n",
    "        image_name = os.path.basename(result[\"image_path\"])[:28]\n",
    "        action = result[\"action\"]\n",
    "        nsfw_p = f\"{result['nsfw_p']:.3f}\"\n",
    "        \n",
    "        matches = result.get(\"matches\", [])\n",
    "        min_dist = str(min([d for _, d in matches], default=\"N/A\"))\n",
    "        \n",
    "        time_ms = str(result.get(\"processing_time_ms\", 0))\n",
    "        \n",
    "        print(f\"{image_name:<30} {action:<10} {nsfw_p:<8} {min_dist:<8} {time_ms:<10}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def create_test_image(path, text=None, color=(255, 255, 255), size=(200, 100)):\n",
    "    from PIL import ImageDraw, ImageFont\n",
    "    img = Image.new('RGB', size, color)\n",
    "    if text:\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"/System/Library/Fonts/Arial.ttf\", 20)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        draw.text((10, 40), text, fill=(0, 0, 0), font=font)\n",
    "    img.save(path)\n",
    "\n",
    "os.makedirs('test_images', exist_ok=True)\n",
    "create_test_image('test_images/clean.jpg', 'Hello World')\n",
    "create_test_image('test_images/threat.jpg', 'I will kill you')\n",
    "create_test_image('test_images/sextortion.jpg', 'Pay me bitcoin or I leak')\n",
    "\n",
    "test_paths = ['test_images/clean.jpg', 'test_images/threat.jpg', 'test_images/sextortion.jpg']\n",
    "print(\"Testing batch decision on sample images:\")\n",
    "batch_results = batch_decide(test_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "os.makedirs('test_images', exist_ok=True)\ncreate_test_image('test_images/clean.jpg', 'Hello World')\ncreate_test_image('test_images/threat.jpg', 'I will kill you')\ncreate_test_image('test_images/sextortion.jpg', 'Pay me bitcoin or I leak')\ncreate_test_image('test_images/nsfw.jpg', 'Adult Content', (255, 100, 100))\n\ntest_paths = ['test_images/clean.jpg', 'test_images/threat.jpg', 'test_images/sextortion.jpg', 'test_images/nsfw.jpg']\nprint(\"Testing batch decision on sample images:\")\nbatch_results = batch_decide(test_paths)\n\nprint(\"\\\\nPolicy Summary:\")\npolicy_summary = get_policy_summary()\nfor key, value in policy_summary.items():\n    print(f\"  {key}: {value}\")\n\npolicy_test_results = test_policy_scenarios()\n\nprint(\"\\\\nTesting policy updates...\")\noriginal_policy = POLICY.copy()\nupdate_policy({\"nsfw\": {\"threshold_adult_blur\": 0.8}})\nprint(f\"Updated adult blur threshold to: {POLICY['nsfw']['threshold_adult_blur']}\")\n\nupdated_result = run_decision('test_images/nsfw.jpg')\nprint(f\"New result with updated policy: {updated_result['action']} (was BLUR, now with lower threshold)\")\n\nPOLICY.update(original_policy)\nprint(\"Policy restored to original values\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}