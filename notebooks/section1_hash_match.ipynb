{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Hash Matching & Image Fingerprinting\n",
    "\n",
    "**Goal**: Image ingest → PDQ hashing → near-duplicate matching → hashlist import → quick API surface doc.\n",
    "\n",
    "**Scope**: Days 1-4 of MVP build. Core hashing infrastructure for blocking known harmful content.\n",
    "\n",
    "Build perceptual hash database, implement Hamming distance matching, support external hashlist feeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources & References\n",
    "\n",
    "**PDQ Hashing**:\n",
    "- https://github.com/facebook/ThreatExchange\n",
    "- https://github.com/darwinium-com/pdqhash\n",
    "\n",
    "**Hash Matching & Action (HMA)**:\n",
    "- https://github.com/facebook/ThreatExchange/tree/main/hasher-matcher-actioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !apt-get update -y && apt-get install -y build-essential cmake\n",
    "\n",
    "%pip install --quiet pdqhash opencv-python-headless pillow numpy\n",
    "\n",
    "import pdqhash\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"PDQ version: {pdqhash.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"PIL version: {Image.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
    "LABELS = {\"CSAM\", \"NCII\", \"TERROR\"}\n",
    "\n",
    "def now_ts():\n",
    "    return int(time.time())\n",
    "\n",
    "def ensure_dirs(*dirs):\n",
    "    for d in dirs:\n",
    "        Path(d).mkdir(exist_ok=True)\n",
    "\n",
    "def to_json(obj):\n",
    "    def json_serializer(o):\n",
    "        if isinstance(o, set):\n",
    "            return list(o)\n",
    "        return str(o)\n",
    "    return json.dumps(obj, default=json_serializer, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pdq(image_path):\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            hash_int, quality = pdqhash.compute(img)\n",
    "            hash_hex = f\"{hash_int:064x}\"\n",
    "            return hash_hex, quality\n",
    "    except Exception as e:\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (8, 8))\n",
    "            avg = resized.mean()\n",
    "            bits = (resized > avg).astype(np.uint8)\n",
    "            hash_int = int(''.join(bits.flatten().astype(str)), 2)\n",
    "            hash_hex = f\"{hash_int:016x}\".zfill(64)\n",
    "            return hash_hex, 50\n",
    "        except:\n",
    "            return \"0\" * 64, 0\n",
    "\n",
    "def hex_to_int(hex_str):\n",
    "    return int(hex_str, 16)\n",
    "\n",
    "def hamming_distance_hex(hex1, hex2):\n",
    "    return bin(hex_to_int(hex1) ^ hex_to_int(hex2)).count('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_DB = {}\n",
    "\n",
    "def add_hash(media_id, hash_hex, quality, source=\"user\", labels=None):\n",
    "    if labels is None:\n",
    "        labels = set()\n",
    "    HASH_DB[media_id] = {\n",
    "        \"hash\": hash_hex,\n",
    "        \"quality\": quality,\n",
    "        \"source\": source,\n",
    "        \"labels\": set(labels) if isinstance(labels, (list, tuple)) else labels\n",
    "    }\n",
    "\n",
    "def get_hash_count():\n",
    "    return len(HASH_DB)\n",
    "\n",
    "def get_hash_info(media_id):\n",
    "    return HASH_DB.get(media_id)\n",
    "\n",
    "def clear_hash_db():\n",
    "    HASH_DB.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def ingest_folder(folder_path, source=\"user\", prefix=\"\"):\n    count = 0\n    errors = 0\n    for root, dirs, files in os.walk(folder_path):\n        for file in files:\n            if Path(file).suffix.lower() in IMG_EXTS:\n                image_path = os.path.join(root, file)\n                media_id = f\"{prefix}{Path(file).stem}_{now_ts()}_{count}\"\n                try:\n                    hash_hex, quality = compute_pdq(image_path)\n                    add_hash(media_id, hash_hex, quality, source)\n                    count += 1\n                except Exception as e:\n                    print(f\"Error processing {image_path}: {e}\")\n                    errors += 1\n    return count, errors\n\ndef match_hash(query_hash, max_distance=30, topk=50):\n    matches = []\n    for media_id, data in HASH_DB.items():\n        distance = hamming_distance_hex(query_hash, data[\"hash\"])\n        if distance <= max_distance:\n            matches.append((media_id, distance))\n    \n    matches.sort(key=lambda x: x[1])\n    return matches[:topk]\n\ndef match_hash_optimized(query_hash, max_distance=30, topk=50):\n    query_int = hex_to_int(query_hash)\n    matches = []\n    \n    items = list(HASH_DB.items())\n    chunk_size = min(1000, len(items))\n    \n    for i in range(0, len(items), chunk_size):\n        chunk = items[i:i + chunk_size]\n        \n        for media_id, data in chunk:\n            hash_int = hex_to_int(data[\"hash\"])\n            distance = bin(query_int ^ hash_int).count('1')\n            if distance <= max_distance:\n                matches.append((media_id, distance))\n                \n        if len(matches) > topk * 2:\n            matches.sort(key=lambda x: x[1])\n            matches = matches[:topk]\n    \n    matches.sort(key=lambda x: x[1])\n    return matches[:topk]\n\ndef benchmark_matching(num_hashes=1000, num_queries=100):\n    print(f\"\\\\nBenchmarking with {num_hashes} hashes and {num_queries} queries...\")\n    \n    original_db = HASH_DB.copy()\n    clear_hash_db()\n    \n    for i in range(num_hashes):\n        hash_hex = f\"{i:016x}\" + \"0\" * 48\n        add_hash(f\"bench_{i}\", hash_hex, 90, \"benchmark\", {\"TEST\"} if i % 10 == 0 else set())\n    \n    test_hashes = [f\"{i*17:016x}\" + \"0\" * 48 for i in range(num_queries)]\n    \n    start_time = time.time()\n    for query_hash in test_hashes:\n        matches = match_hash(query_hash, max_distance=20, topk=10)\n    brute_time = (time.time() - start_time) * 1000\n    \n    start_time = time.time()\n    for query_hash in test_hashes:\n        matches = match_hash_optimized(query_hash, max_distance=20, topk=10)\n    optimized_time = (time.time() - start_time) * 1000\n    \n    speedup = brute_time / optimized_time if optimized_time > 0 else float('inf')\n    \n    print(f\"  Brute force: {brute_time:.1f}ms ({brute_time/num_queries:.2f}ms per query)\")\n    print(f\"  Optimized: {optimized_time:.1f}ms ({optimized_time/num_queries:.2f}ms per query)\")\n    print(f\"  Speedup: {speedup:.1f}x\")\n    \n    HASH_DB.clear()\n    HASH_DB.update(original_db)\n    \n    return {\"brute_force_ms\": brute_time, \"optimized_ms\": optimized_time, \"speedup\": speedup}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_csv_hashlist(csv_path, source):\n",
    "    count = 0\n",
    "    with open(csv_path, 'r', newline='') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            hash_hex = row.get('hash_hex', '').strip()\n",
    "            label = row.get('label', '').strip()\n",
    "            \n",
    "            if not hash_hex or len(hash_hex) != 64:\n",
    "                continue\n",
    "                \n",
    "            media_id = f\"{source}:{hash_hex[:12]}\"\n",
    "            \n",
    "            if media_id in HASH_DB:\n",
    "                HASH_DB[media_id][\"labels\"].add(label)\n",
    "            else:\n",
    "                add_hash(media_id, hash_hex, 100, source, {label} if label else set())\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "def export_hashlist_csv(output_path):\n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['media_id', 'hash_hex', 'quality', 'source', 'labels'])\n",
    "        for media_id, data in HASH_DB.items():\n",
    "            labels_str = ','.join(data['labels']) if data['labels'] else ''\n",
    "            writer.writerow([media_id, data['hash'], data['quality'], data['source'], labels_str])\n",
    "    return len(HASH_DB)\n",
    "\n",
    "def create_sample_hashlist(output_path):\n",
    "    sample_data = [\n",
    "        ['hash_hex', 'label'],\n",
    "        ['1234567890abcdef' + '0' * 48, 'CSAM'],\n",
    "        ['fedcba0987654321' + '0' * 48, 'TERROR'],\n",
    "        ['abcdef1234567890' + '0' * 48, 'NCII']\n",
    "    ]\n",
    "    \n",
    "    with open(output_path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(sample_data)\n",
    "    return len(sample_data) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "ensure_dirs('samples')\n\ndef create_sample_image(path, color, size=(100, 100)):\n    img = Image.new('RGB', size, color)\n    img.save(path)\n\nsample_paths = [\n    'samples/red_square.jpg',\n    'samples/blue_square.jpg'\n]\n\ncreate_sample_image(sample_paths[0], (255, 0, 0))\ncreate_sample_image(sample_paths[1], (0, 0, 255))\n\ningested, errors = ingest_folder('samples', 'demo')\nprint(f\"Ingested {ingested} sample images ({errors} errors)\")\n\nif sample_paths[0] and os.path.exists(sample_paths[0]):\n    demo_hash, demo_quality = compute_pdq(sample_paths[0])\n    print(f\"Demo PDQ hash: {demo_hash}\")\n    print(f\"Demo quality: {demo_quality}\")\n    \n    matches = match_hash(demo_hash, max_distance=10)\n    print(f\"Matches (distance ≤ 10): {matches}\")\n\nsample_csv = 'samples/sample_hashes.csv'\ncreated_hashes = create_sample_hashlist(sample_csv)\nimported_hashes = import_csv_hashlist(sample_csv, 'threat_feed')\nprint(f\"Created and imported {imported_hashes} threat hashes\")\n\nprint(f\"\\\\nTotal hashes in DB: {get_hash_count()}\")\nfor media_id, data in list(HASH_DB.items())[:3]:\n    print(f\"  {media_id}: {data['hash'][:16]}... (labels: {data['labels']})\")\n\nbench_result = benchmark_matching(100, 20)\nprint(f\"\\\\nPerformance benchmark complete: {bench_result['speedup']:.1f}x speedup\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Surface (For Integration)\n",
    "\n",
    "### POST /hash\n",
    "Compute hash for uploaded image\n",
    "```json\n",
    "{\n",
    "  \"image\": \"<base64_encoded_image>\",\n",
    "  \"media_id\": \"optional_id\"\n",
    "}\n",
    "```\n",
    "Response:\n",
    "```json\n",
    "{\n",
    "  \"hash_hex\": \"abc123...\",\n",
    "  \"quality\": 85,\n",
    "  \"media_id\": \"img_1234567890\"\n",
    "}\n",
    "```\n",
    "\n",
    "### POST /match\n",
    "Find similar hashes\n",
    "```json\n",
    "{\n",
    "  \"hash_hex\": \"abc123...\",\n",
    "  \"max_distance\": 30,\n",
    "  \"topk\": 50\n",
    "}\n",
    "```\n",
    "Response:\n",
    "```json\n",
    "{\n",
    "  \"matches\": [\n",
    "    {\"media_id\": \"known_bad_123\", \"distance\": 5, \"labels\": [\"CSAM\"]},\n",
    "    {\"media_id\": \"similar_456\", \"distance\": 12, \"labels\": []}\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day-by-Day Checklist (Days 1-4)\n",
    "\n",
    "### Day 1: Core Hashing\n",
    "- [x] Set up PDQ hashing with fallback to aHash\n",
    "- [x] Implement Hamming distance calculation\n",
    "- [x] Create in-memory hash database\n",
    "- [x] Test with sample images\n",
    "\n",
    "### Day 2: Matching & Performance\n",
    "- [x] Implement brute-force hash matching\n",
    "- [x] Add optimized matching with chunked processing\n",
    "- [x] Test matching accuracy and performance\n",
    "- [ ] Benchmark with larger datasets (1000+ hashes)\n",
    "\n",
    "### Day 3: Import/Export\n",
    "- [x] CSV hashlist import functionality\n",
    "- [x] CSV export for backup\n",
    "- [x] Sample threat hash creation\n",
    "- [ ] Integration with external hash feeds (HMA/ThreatExchange)\n",
    "\n",
    "### Day 4: Integration Prep\n",
    "- [x] API specification documentation\n",
    "- [ ] Performance optimization for production scale\n",
    "- [ ] Error handling improvements\n",
    "- [ ] Integration testing with Section 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}