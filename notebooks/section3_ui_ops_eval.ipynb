{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: UI, Operations & Evaluation\n",
    "\n",
    "**Goal**: Reviewer widget, optional FastAPI, metrics logging, eval harness, adversarial tests, rollout plan, acceptance criteria.\n",
    "\n",
    "**Purpose**: Days 6-10 operational tooling for production deployment.\n",
    "\n",
    "**Scope**: Human review interface, API deployment, metrics collection, and system validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet pillow numpy fastapi uvicorn python-multipart jinja2\n",
    "\n",
    "from PIL import Image, ImageFilter, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from pathlib import Path\n",
    "import statistics\n",
    "from fastapi import FastAPI, File, UploadFile, Form\n",
    "from fastapi.responses import JSONResponse, HTMLResponse\n",
    "import uvicorn\n",
    "import threading\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "print(f\"PIL version: {Image.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"FastAPI imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASH_DB = {\n",
    "    \"threat_feed:1234567890ab\": {\"hash\": \"1234567890abcdef\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"CSAM\"}},\n",
    "    \"threat_feed:fedcba098765\": {\"hash\": \"fedcba0987654321\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"TERROR\"}},\n",
    "    \"threat_feed:abcdef123456\": {\"hash\": \"abcdef1234567890\" + \"0\" * 48, \"quality\": 100, \"labels\": {\"NCII\"}}\n",
    "}\n",
    "\n",
    "POLICY = {\n",
    "    \"hash\": {\"max_distance\": 30},\n",
    "    \"nsfw\": {\"threshold_adult_blur\": 0.95, \"threshold_teen_block\": 0.85},\n",
    "    \"ocr\": {\"enable\": True, \"require_review_on_hate\": True, \"require_review_on_threat\": True, \"require_review_on_sextortion\": True},\n",
    "    \"age\": {\"teen_mode\": False}\n",
    "}\n",
    "\n",
    "def compute_pdq(image_path):\n",
    "    import hashlib\n",
    "    with open(image_path, 'rb') as f:\n",
    "        content = f.read()\n",
    "    hash_hex = hashlib.md5(content).hexdigest().ljust(64, '0')\n",
    "    return hash_hex, 50\n",
    "\n",
    "def hamming_distance_hex(hex1, hex2):\n",
    "    return bin(int(hex1, 16) ^ int(hex2, 16)).count('1')\n",
    "\n",
    "def match_hash(query_hash, max_distance=30, topk=50):\n",
    "    matches = []\n",
    "    for media_id, data in HASH_DB.items():\n",
    "        distance = hamming_distance_hex(query_hash, data[\"hash\"])\n",
    "        if distance <= max_distance:\n",
    "            matches.append((media_id, distance, data.get(\"labels\", set())))\n",
    "    matches.sort(key=lambda x: x[1])\n",
    "    return matches[:topk]\n",
    "\n",
    "def score_nsfw(image_path):\n",
    "    return np.random.random() * 0.4\n",
    "\n",
    "def extract_text(image_path):\n",
    "    return \"\"\n",
    "\n",
    "HATE_KEYWORDS = [\"slur1\", \"slur2\", \"nazi\", \"terrorist\", \"hate\"]\n",
    "THREAT_KEYWORDS = [\"kill\", \"shoot\", \"bomb\", \"murder\", \"attack\"]\n",
    "SEXTORTION_KEYWORDS = [\"pay\", \"bitcoin\", \"leak\", \"expose\", \"money\", \"send\"]\n",
    "\n",
    "def flag_keywords(text):\n",
    "    if not text:\n",
    "        return {\"hate\": False, \"threat\": False, \"sextortion\": False}\n",
    "    \n",
    "    text_lower = text.lower()\n",
    "    return {\n",
    "        \"hate\": any(keyword in text_lower for keyword in HATE_KEYWORDS),\n",
    "        \"threat\": any(keyword in text_lower for keyword in THREAT_KEYWORDS),\n",
    "        \"sextortion\": any(keyword in text_lower for keyword in SEXTORTION_KEYWORDS)\n",
    "    }\n",
    "\n",
    "def decide(image_path, matches, nsfw_p, ocr_text, flags, teen_mode=False):\n",
    "    reasons = []\n",
    "    action = \"ALLOW\"\n",
    "    \n",
    "    harmful_labels = {\"CSAM\", \"NCII\", \"TERROR\"}\n",
    "    for media_id, distance, labels in matches:\n",
    "        if distance <= POLICY[\"hash\"][\"max_distance\"] and labels & harmful_labels:\n",
    "            action = \"BLOCK\"\n",
    "            reasons.append(f\"Hash match: {list(labels)} (distance: {distance})\")\n",
    "            break\n",
    "    \n",
    "    if action == \"ALLOW\":\n",
    "        if teen_mode and nsfw_p >= POLICY[\"nsfw\"][\"threshold_teen_block\"]:\n",
    "            action = \"BLOCK\"\n",
    "            reasons.append(f\"Teen NSFW block (score: {nsfw_p:.3f})\")\n",
    "        elif nsfw_p >= POLICY[\"nsfw\"][\"threshold_adult_blur\"]:\n",
    "            action = \"BLUR\"\n",
    "            reasons.append(f\"Adult NSFW blur (score: {nsfw_p:.3f})\")\n",
    "    \n",
    "    if action == \"ALLOW\" and POLICY[\"ocr\"][\"enable\"] and any(flags.values()):\n",
    "        action = \"REVIEW\"\n",
    "        flag_types = [k for k, v in flags.items() if v]\n",
    "        reasons.append(f\"OCR flags: {flag_types}\")\n",
    "    \n",
    "    return {\n",
    "        \"action\": action,\n",
    "        \"reasons\": reasons,\n",
    "        \"nsfw_p\": nsfw_p,\n",
    "        \"matches\": [(mid, dist) for mid, dist, _ in matches],\n",
    "        \"ocr_excerpt\": ocr_text[:100] + \"...\" if len(ocr_text) > 100 else ocr_text\n",
    "    }\n",
    "\n",
    "def run_decision(image_path, max_distance=None, teen_mode=None):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    hash_hex, quality = compute_pdq(image_path)\n",
    "    matches = match_hash(hash_hex, max_distance or 30)\n",
    "    nsfw_p = score_nsfw(image_path)\n",
    "    ocr_text = extract_text(image_path)\n",
    "    flags = flag_keywords(ocr_text)\n",
    "    decision = decide(image_path, matches, nsfw_p, ocr_text, flags, teen_mode or False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"hash_hex\": hash_hex,\n",
    "        \"processing_time_ms\": int((end_time - start_time) * 1000),\n",
    "        \"teen_mode\": teen_mode or False,\n",
    "        \"ocr_flags\": flags,\n",
    "        **decision\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_image_colab():\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        from IPython.display import display, HTML, Image as IPImage\n",
    "        \n",
    "        print(\"Upload an image for moderation review:\")\n",
    "        uploaded = files.upload()\n",
    "        \n",
    "        for filename in uploaded.keys():\n",
    "            print(f\"\\nProcessing: {filename}\")\n",
    "            \n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(uploaded[filename])\n",
    "            \n",
    "            result = run_decision(filename)\n",
    "            \n",
    "            print(f\"Action: {result['action']}\")\n",
    "            print(f\"NSFW Score: {result['nsfw_p']:.3f}\")\n",
    "            print(f\"Reasons: {result['reasons']}\")\n",
    "            print(f\"Processing Time: {result['processing_time_ms']}ms\")\n",
    "            print(f\"OCR Flags: {result['ocr_flags']}\")\n",
    "            \n",
    "            if result['action'] == 'BLUR':\n",
    "                img = Image.open(filename)\n",
    "                blurred = img.filter(ImageFilter.GaussianBlur(radius=10))\n",
    "                blurred_path = f\"blurred_{filename}\"\n",
    "                blurred.save(blurred_path)\n",
    "                \n",
    "                display(HTML(\"<h3>Blurred Preview (Adult Content Detected):</h3>\"))\n",
    "                display(IPImage(blurred_path, width=300))\n",
    "                \n",
    "                display(HTML(f\"\"\"\n",
    "                <button onclick=\"\n",
    "                    document.getElementById('blurred_{filename}').style.display='none';\n",
    "                    document.getElementById('original_{filename}').style.display='block';\n",
    "                \">Click to Reveal Original</button>\n",
    "                <div id=\"blurred_{filename}\">\n",
    "                    <p>Content blurred due to NSFW detection</p>\n",
    "                </div>\n",
    "                <div id=\"original_{filename}\" style=\"display:none\">\n",
    "                    <img src=\"{filename}\" width=\"300\">\n",
    "                    <p>Original image revealed</p>\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "                \n",
    "                os.unlink(blurred_path)\n",
    "            \n",
    "            elif result['action'] == 'BLOCK':\n",
    "                display(HTML(f\"\"\"\n",
    "                <div style=\"background-color: #ffebee; padding: 20px; border: 2px solid #f44336;\">\n",
    "                    <h3 style=\"color: #d32f2f;\">‚ö†Ô∏è CONTENT BLOCKED</h3>\n",
    "                    <p><strong>Reason:</strong> {'; '.join(result['reasons'])}</p>\n",
    "                    <p>This content violates community guidelines and cannot be displayed.</p>\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "            \n",
    "            elif result['action'] == 'REVIEW':\n",
    "                display(HTML(f\"\"\"\n",
    "                <div style=\"background-color: #fff3e0; padding: 20px; border: 2px solid #ff9800;\">\n",
    "                    <h3 style=\"color: #f57c00;\">üìã FLAGGED FOR REVIEW</h3>\n",
    "                    <p><strong>Reason:</strong> {'; '.join(result['reasons'])}</p>\n",
    "                    <img src=\"{filename}\" width=\"300\" style=\"opacity: 0.7;\">\n",
    "                    <p>Content flagged for human review</p>\n",
    "                </div>\n",
    "                \"\"\"))\n",
    "            \n",
    "            else:\n",
    "                display(HTML(\"<h3>‚úÖ Content Approved:</h3>\"))\n",
    "                display(IPImage(filename, width=300))\n",
    "            \n",
    "            os.unlink(filename)\n",
    "                \n",
    "    except ImportError:\n",
    "        print(\"This function requires Google Colab environment\")\n",
    "        print(\"In other environments, use the FastAPI endpoints instead\")\n",
    "        return None\n",
    "\n",
    "print(\"Reviewer widget ready. Call review_image_colab() to start.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Image Moderation API\", version=\"1.0.0\")\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return HTMLResponse(\"\"\"\n",
    "    <html><head><title>Image Moderation API</title></head><body>\n",
    "    <h1>Image Moderation API</h1>\n",
    "    <p>Available endpoints:</p>\n",
    "    <ul>\n",
    "        <li><a href=\"/docs\">API Documentation</a></li>\n",
    "        <li>POST /hash - Compute image hash</li>\n",
    "        <li>POST /match - Find hash matches</li>\n",
    "        <li>POST /classify/nsfw - NSFW classification</li>\n",
    "        <li>POST /ocr - Text extraction</li>\n",
    "        <li>POST /decide - Complete moderation decision</li>\n",
    "    </ul>\n",
    "    </body></html>\n",
    "    \"\"\")\n",
    "\n",
    "@app.post(\"/hash\")\n",
    "async def hash_image(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    temp_path = f\"temp_{file.filename}\"\n",
    "    \n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    try:\n",
    "        hash_hex, quality = compute_pdq(temp_path)\n",
    "        return {\"hash_hex\": hash_hex, \"quality\": quality, \"filename\": file.filename}\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "@app.post(\"/match\")\n",
    "async def match_hash_api(hash_hex: str = Form(...), max_distance: int = Form(30)):\n",
    "    matches = match_hash(hash_hex, max_distance)\n",
    "    return {\n",
    "        \"matches\": [\n",
    "            {\"media_id\": mid, \"distance\": dist, \"labels\": list(labels)} \n",
    "            for mid, dist, labels in matches\n",
    "        ]\n",
    "    }\n",
    "\n",
    "@app.post(\"/classify/nsfw\")\n",
    "async def classify_nsfw(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    temp_path = f\"temp_{file.filename}\"\n",
    "    \n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    try:\n",
    "        nsfw_score = score_nsfw(temp_path)\n",
    "        return {\"nsfw_probability\": nsfw_score, \"filename\": file.filename}\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "@app.post(\"/ocr\")\n",
    "async def ocr_image(file: UploadFile = File(...)):\n",
    "    content = await file.read()\n",
    "    temp_path = f\"temp_{file.filename}\"\n",
    "    \n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    try:\n",
    "        text = extract_text(temp_path)\n",
    "        flags = flag_keywords(text)\n",
    "        return {\"text\": text, \"flags\": flags, \"filename\": file.filename}\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "@app.post(\"/decide\")\n",
    "async def decide_image(file: UploadFile = File(...), teen_mode: bool = Form(False)):\n",
    "    content = await file.read()\n",
    "    temp_path = f\"temp_{file.filename}\"\n",
    "    \n",
    "    with open(temp_path, \"wb\") as f:\n",
    "        f.write(content)\n",
    "    \n",
    "    try:\n",
    "        result = run_decision(temp_path, teen_mode=teen_mode)\n",
    "        result[\"filename\"] = file.filename\n",
    "        return result\n",
    "    finally:\n",
    "        if os.path.exists(temp_path):\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "def start_server(host=\"0.0.0.0\", port=8000):\n",
    "    print(f\"Starting FastAPI server on {host}:{port}\")\n",
    "    print(f\"API Documentation: http://localhost:{port}/docs\")\n",
    "    print(f\"API Root: http://localhost:{port}/\")\n",
    "    uvicorn.run(app, host=host, port=port, log_level=\"info\")\n",
    "\n",
    "print(\"FastAPI app configured. Call start_server() to launch (not started by default).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"data\").mkdir(exist_ok=True)\n",
    "METRICS_FILE = \"data/metrics.csv\"\n",
    "\n",
    "def log_decision(result, teen_mode=False):\n",
    "    fieldnames = ['ts', 'image', 'action', 'nsfw_p', 'reasons', 'hash_min_distance', 'teen_mode', 'processing_time_ms']\n",
    "    \n",
    "    file_exists = os.path.exists(METRICS_FILE)\n",
    "    \n",
    "    with open(METRICS_FILE, 'a', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        min_distance = min([d for _, d in result.get('matches', [])], default='N/A')\n",
    "        reasons_str = '; '.join(result.get('reasons', []))\n",
    "        \n",
    "        writer.writerow({\n",
    "            'ts': time.time(),\n",
    "            'image': os.path.basename(result.get('image_path', 'unknown')),\n",
    "            'action': result.get('action', 'UNKNOWN'),\n",
    "            'nsfw_p': result.get('nsfw_p', 0.0),\n",
    "            'reasons': reasons_str,\n",
    "            'hash_min_distance': min_distance,\n",
    "            'teen_mode': teen_mode,\n",
    "            'processing_time_ms': result.get('processing_time_ms', 0)\n",
    "        })\n",
    "\n",
    "def show_recent_metrics(n=10):\n",
    "    if not os.path.exists(METRICS_FILE):\n",
    "        print(\"No metrics file found. Use log_decision() to start logging.\")\n",
    "        return\n",
    "    \n",
    "    with open(METRICS_FILE, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    if not rows:\n",
    "        print(\"No metrics data found.\")\n",
    "        return\n",
    "    \n",
    "    recent = rows[-n:]\n",
    "    \n",
    "    print(f\"\\nLast {len(recent)} decisions:\")\n",
    "    print(f\"{'Image':<20} {'Action':<10} {'NSFW':<8} {'Time(ms)':<10} {'Reasons':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for row in recent:\n",
    "        image = row['image'][:18]\n",
    "        action = row['action']\n",
    "        nsfw_p = f\"{float(row['nsfw_p']):.3f}\"\n",
    "        time_ms = row['processing_time_ms']\n",
    "        reasons = row['reasons'][:28]\n",
    "        \n",
    "        print(f\"{image:<20} {action:<10} {nsfw_p:<8} {time_ms:<10} {reasons:<30}\")\n",
    "\n",
    "def get_metrics_summary():\n",
    "    if not os.path.exists(METRICS_FILE):\n",
    "        return {\"error\": \"No metrics file found\"}\n",
    "    \n",
    "    with open(METRICS_FILE, 'r') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        rows = list(reader)\n",
    "    \n",
    "    if not rows:\n",
    "        return {\"error\": \"No metrics data\"}\n",
    "    \n",
    "    actions = [row['action'] for row in rows]\n",
    "    processing_times = [float(row['processing_time_ms']) for row in rows if row['processing_time_ms']]\n",
    "    \n",
    "    action_counts = {}\n",
    "    for action in actions:\n",
    "        action_counts[action] = action_counts.get(action, 0) + 1\n",
    "    \n",
    "    return {\n",
    "        \"total_decisions\": len(rows),\n",
    "        \"action_breakdown\": action_counts,\n",
    "        \"avg_processing_time_ms\": statistics.mean(processing_times) if processing_times else 0,\n",
    "        \"p95_processing_time_ms\": np.percentile(processing_times, 95) if processing_times else 0,\n",
    "        \"automation_rate\": (action_counts.get('ALLOW', 0) + action_counts.get('BLUR', 0) + action_counts.get('BLOCK', 0)) / len(rows) if rows else 0\n",
    "    }\n",
    "\n",
    "print(\"Metrics logging ready. Use log_decision(result) and show_recent_metrics().\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def get_metrics_summary():\n    if not os.path.exists(METRICS_FILE):\n        return {\"error\": \"No metrics file found\"}\n    \n    with open(METRICS_FILE, 'r') as f:\n        reader = csv.DictReader(f)\n        rows = list(reader)\n    \n    if not rows:\n        return {\"error\": \"No metrics data\"}\n    \n    actions = [row['action'] for row in rows]\n    processing_times = [float(row['processing_time_ms']) for row in rows if row['processing_time_ms']]\n    teen_decisions = [row for row in rows if row['teen_mode'].lower() == 'true']\n    \n    action_counts = {}\n    for action in actions:\n        action_counts[action] = action_counts.get(action, 0) + 1\n    \n    hourly_counts = {}\n    for row in rows:\n        try:\n            hour = int(float(row['ts']) // 3600)\n            hourly_counts[hour] = hourly_counts.get(hour, 0) + 1\n        except:\n            pass\n    \n    return {\n        \"total_decisions\": len(rows),\n        \"action_breakdown\": action_counts,\n        \"teen_decisions\": len(teen_decisions),\n        \"avg_processing_time_ms\": statistics.mean(processing_times) if processing_times else 0,\n        \"p95_processing_time_ms\": np.percentile(processing_times, 95) if processing_times else 0,\n        \"p99_processing_time_ms\": np.percentile(processing_times, 99) if processing_times else 0,\n        \"automation_rate\": (action_counts.get('ALLOW', 0) + action_counts.get('BLUR', 0) + action_counts.get('BLOCK', 0)) / len(rows) if rows else 0,\n        \"zero_view_rate\": action_counts.get('BLOCK', 0) / len(rows) if rows else 0,\n        \"human_review_rate\": action_counts.get('REVIEW', 0) / len(rows) if rows else 0,\n        \"hourly_volume\": hourly_counts\n    }\n\ndef generate_transparency_report():\n    summary = get_metrics_summary()\n    \n    if \"error\" in summary:\n        return summary\n    \n    report = {\n        \"reporting_period\": f\"Last {summary['total_decisions']} decisions\",\n        \"proactive_detection\": {\n            \"automation_rate\": f\"{summary['automation_rate']*100:.1f}%\",\n            \"zero_view_rate\": f\"{summary['zero_view_rate']*100:.1f}%\",\n            \"avg_decision_time\": f\"{summary['avg_processing_time_ms']:.1f}ms\"\n        },\n        \"content_actions\": {\n            \"allowed\": summary['action_breakdown'].get('ALLOW', 0),\n            \"blurred\": summary['action_breakdown'].get('BLUR', 0),\n            \"blocked\": summary['action_breakdown'].get('BLOCK', 0),\n            \"reviewed\": summary['action_breakdown'].get('REVIEW', 0)\n        },\n        \"teen_safety\": {\n            \"teen_decisions\": summary['teen_decisions'],\n            \"teen_protection_rate\": f\"{(summary['teen_decisions'] / summary['total_decisions'] * 100):.1f}%\" if summary['total_decisions'] > 0 else \"0%\"\n        },\n        \"performance\": {\n            \"p95_latency_ms\": summary['p95_processing_time_ms'],\n            \"p99_latency_ms\": summary['p99_processing_time_ms'],\n            \"sla_compliance\": \"‚úÖ\" if summary['p95_processing_time_ms'] < 250 else \"‚ùå\"\n        }\n    }\n    \n    return report\n\ndef advanced_evaluation(test_scenarios=None):\n    if test_scenarios is None:\n        test_scenarios = [\n            {\"path\": \"eval_images/clean.jpg\", \"expected\": \"ALLOW\", \"surface\": \"feed\", \"teen\": False},\n            {\"path\": \"eval_images/nsfw.jpg\", \"expected\": \"BLUR\", \"surface\": \"feed\", \"teen\": False},\n            {\"path\": \"eval_images/nsfw.jpg\", \"expected\": \"BLOCK\", \"surface\": \"avatar\", \"teen\": True},\n            {\"path\": \"eval_images/threat.jpg\", \"expected\": \"REVIEW\", \"surface\": \"dm\", \"teen\": False},\n        ]\n    \n    results = []\n    confusion_matrix = {\"TP\": 0, \"TN\": 0, \"FP\": 0, \"FN\": 0}\n    \n    print(f\"\\\\nRunning advanced evaluation on {len(test_scenarios)} scenarios...\")\n    \n    for scenario in test_scenarios:\n        if not os.path.exists(scenario[\"path\"]):\n            continue\n            \n        start_time = time.time()\n        result = run_decision(\n            scenario[\"path\"], \n            teen_mode=scenario[\"teen\"],\n            surface=scenario[\"surface\"]\n        )\n        latency = (time.time() - start_time) * 1000\n        \n        correct = result[\"action\"] == scenario[\"expected\"]\n        \n        if correct:\n            if scenario[\"expected\"] in [\"BLOCK\", \"REVIEW\"]:\n                confusion_matrix[\"TP\"] += 1\n            else:\n                confusion_matrix[\"TN\"] += 1\n        else:\n            if scenario[\"expected\"] in [\"BLOCK\", \"REVIEW\"]:\n                confusion_matrix[\"FN\"] += 1\n            else:\n                confusion_matrix[\"FP\"] += 1\n        \n        scenario_result = {\n            \"scenario\": os.path.basename(scenario[\"path\"]),\n            \"surface\": scenario[\"surface\"],\n            \"teen_mode\": scenario[\"teen\"],\n            \"expected\": scenario[\"expected\"],\n            \"actual\": result[\"action\"],\n            \"correct\": correct,\n            \"latency_ms\": latency,\n            \"confidence\": result.get(\"confidence\", 0.0)\n        }\n        \n        results.append(scenario_result)\n        \n        status = \"‚úÖ\" if correct else \"‚ùå\"\n        print(f\"  {status} {scenario_result['scenario']} ({scenario_result['surface']}, {'teen' if scenario_result['teen_mode'] else 'adult'}): {scenario_result['actual']} (expected {scenario_result['expected']})\")\n    \n    if results:\n        accuracy = sum(r[\"correct\"] for r in results) / len(results)\n        avg_latency = statistics.mean([r[\"latency_ms\"] for r in results])\n        \n        tp, tn, fp, fn = confusion_matrix[\"TP\"], confusion_matrix[\"TN\"], confusion_matrix[\"FP\"], confusion_matrix[\"FN\"]\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        evaluation_summary = {\n            \"accuracy\": accuracy,\n            \"precision\": precision,\n            \"recall\": recall,\n            \"f1_score\": f1_score,\n            \"avg_latency_ms\": avg_latency,\n            \"scenarios_tested\": len(results),\n            \"confusion_matrix\": confusion_matrix\n        }\n        \n        print(f\"\\\\nEvaluation Summary:\")\n        print(f\"  Accuracy: {accuracy:.3f}\")\n        print(f\"  Precision: {precision:.3f}\")\n        print(f\"  Recall: {recall:.3f}\")\n        print(f\"  F1 Score: {f1_score:.3f}\")\n        print(f\"  Avg Latency: {avg_latency:.1f}ms\")\n        \n        return evaluation_summary\n    \n    return {\"error\": \"No valid scenarios found\"}"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_image_basic(image_path, output_dir=\"augmented\"):\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        base_name = Path(image_path).stem\n",
    "        augmented_paths = []\n",
    "        \n",
    "        crop_w, crop_h = int(img.width * 0.8), int(img.height * 0.8)\n",
    "        left = (img.width - crop_w) // 2\n",
    "        top = (img.height - crop_h) // 2\n",
    "        cropped = img.crop((left, top, left + crop_w, top + crop_h))\n",
    "        crop_path = f\"{output_dir}/{base_name}_crop.jpg\"\n",
    "        cropped.save(crop_path, quality=95)\n",
    "        augmented_paths.append(crop_path)\n",
    "        \n",
    "        jpeg_path = f\"{output_dir}/{base_name}_jpeg.jpg\"\n",
    "        img.save(jpeg_path, quality=50)\n",
    "        augmented_paths.append(jpeg_path)\n",
    "        \n",
    "        enhancer = ImageFilter.Color()\n",
    "        saturated = img.filter(ImageFilter.UnsharpMask())\n",
    "        sat_path = f\"{output_dir}/{base_name}_enhanced.jpg\"\n",
    "        saturated.save(sat_path)\n",
    "        augmented_paths.append(sat_path)\n",
    "        \n",
    "        return augmented_paths\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Augmentation failed for {image_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def test_adversarial_robustness(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Image not found: {image_path}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"Testing adversarial robustness for: {image_path}\")\n",
    "    \n",
    "    original_result = run_decision(image_path)\n",
    "    print(f\"Original: {original_result['action']} (NSFW: {original_result['nsfw_p']:.3f})\")\n",
    "    \n",
    "    augmented_paths = augment_image_basic(image_path)\n",
    "    \n",
    "    if not augmented_paths:\n",
    "        print(\"No augmented images created\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'Augmentation':<15} {'Action':<10} {'NSFW Delta':<12} {'Hash Delta':<12}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for aug_path in augmented_paths:\n",
    "        try:\n",
    "            aug_result = run_decision(aug_path)\n",
    "            \n",
    "            nsfw_delta = aug_result['nsfw_p'] - original_result['nsfw_p']\n",
    "            orig_hash_dist = hamming_distance_hex(original_result['hash_hex'], aug_result['hash_hex'])\n",
    "            \n",
    "            aug_type = Path(aug_path).stem.split('_')[-1]\n",
    "            \n",
    "            print(f\"{aug_type:<15} {aug_result['action']:<10} {nsfw_delta:<12.3f} {orig_hash_dist:<12}\")\n",
    "            \n",
    "            if aug_result['action'] != original_result['action']:\n",
    "                print(f\"  ‚ö†Ô∏è  Action changed: {original_result['action']} ‚Üí {aug_result['action']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Path(aug_path).stem:<15} ERROR: {e}\")\n",
    "        \n",
    "        finally:\n",
    "            if os.path.exists(aug_path):\n",
    "                os.unlink(aug_path)\n",
    "\n",
    "def run_adversarial_tests():\n",
    "    if EVAL_SET:\n",
    "        print(\"Running adversarial tests on evaluation set...\")\n",
    "        for test_case in EVAL_SET[:2]:\n",
    "            test_adversarial_robustness(test_case[\"image_path\"])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"No evaluation set found. Use create_eval_images() first.\")\n",
    "\n",
    "print(\"Adversarial testing ready. Use test_adversarial_robustness(image_path) or run_adversarial_tests().\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privacy & Compliance Notes\n",
    "\n",
    "### Data Separation & Access Control\n",
    "- **Hash databases** stored separately from raw media files with different access permissions\n",
    "- **Least privilege access**: Hash matching services cannot access original media\n",
    "- **Immutable audit logs** for all moderation decisions with cryptographic integrity\n",
    "- **Retention policies**: Automatic deletion of temporary files, configurable log retention\n",
    "\n",
    "### Mandatory Reporting Workflows\n",
    "- **NCMEC reporting** templates for US-based CSAM detection with one-click submission\n",
    "- **IWF reporting** stubs for international coordination on child safety\n",
    "- **Evidence preservation**: Secure storage with chain-of-custody logging\n",
    "- **Legal review gates**: Mandatory approval workflow before external reporting\n",
    "\n",
    "### Privacy-by-Design Architecture\n",
    "- **StopNCII client hashes**: Users submit hashes from devices, not actual content\n",
    "- **No raw biometrics storage**: Age verification uses vendor attestation tokens only\n",
    "- **Age gates without profiling**: Content restrictions based on declared age bands\n",
    "- **On-device processing**: DM nudity detection happens locally when possible\n",
    "- **Differential privacy**: Statistical queries over aggregated decision data\n",
    "\n",
    "### Regulatory Compliance\n",
    "- **GDPR Article 22**: Right to explanation for automated decision-making\n",
    "- **COPPA compliance**: Enhanced protections and parental controls for under-13 users\n",
    "- **DSA transparency**: Quarterly reporting on content moderation metrics for EU\n",
    "- **Section 230 safe harbor**: Good faith moderation efforts with human oversight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workstreams & Owners\n",
    "\n",
    "**Hash Infrastructure** ‚Üí *Backend Lead*: PDQ deployment, MIH indexing, ThreatExchange integration, performance optimization\n",
    "\n",
    "**ML Model Operations** ‚Üí *ML Engineer*: NSFW/OCR model optimization, threshold tuning, A/B testing, drift monitoring\n",
    "\n",
    "**Decision Engine** ‚Üí *Product Engineering*: Policy rules engine, surface-specific configs, appeals integration\n",
    "\n",
    "**Reviewer Tools** ‚Üí *Frontend Lead*: Human review interface, queue management, SLA monitoring, escalation workflows\n",
    "\n",
    "**API & Infrastructure** ‚Üí *DevOps Lead*: FastAPI deployment, load balancing, monitoring, security hardening\n",
    "\n",
    "**Privacy & Legal Compliance** ‚Üí *Trust & Safety*: GDPR/COPPA compliance, reporting workflows, data governance\n",
    "\n",
    "**Metrics & Analytics** ‚Üí *Data Science*: Performance tracking, accuracy measurement, adversarial testing\n",
    "\n",
    "**Teen Safety Operations** ‚Üí *Youth Safety PM*: Age verification flows, content level policies, parental tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Week Schedule & Acceptance Criteria\n",
    "\n",
    "### Days 1-4: Foundation (Hash Infrastructure)\n",
    "- ‚úÖ PDQ hashing with aHash fallback\n",
    "- ‚úÖ In-memory hash database with Hamming distance matching\n",
    "- ‚úÖ CSV hashlist import/export functionality\n",
    "- ‚úÖ Sample data generation and basic API specification\n",
    "- üéØ **Target**: Process 1000+ images/minute with <30ms hash computation\n",
    "\n",
    "### Days 5-7: Signals (ML Classification)\n",
    "- ‚úÖ NSFW classification using OpenNSFW2 (CPU optimized)\n",
    "- ‚úÖ OCR text extraction with PaddleOCR (English)\n",
    "- ‚úÖ Keyword flagging for hate/threat/sextortion content\n",
    "- ‚úÖ Policy rules engine with configurable thresholds\n",
    "- üéØ **Target**: End-to-end decision in <250ms P95 latency\n",
    "\n",
    "### Days 8-10: Operations (UI & Deployment)\n",
    "- ‚úÖ Colab reviewer widget with blur/block/review workflows\n",
    "- ‚úÖ FastAPI deployment with full endpoint coverage\n",
    "- ‚úÖ CSV metrics logging with performance tracking\n",
    "- ‚úÖ Evaluation harness with threshold optimization\n",
    "- ‚úÖ Adversarial testing with image augmentation\n",
    "- üéØ **Target**: Production-ready deployment with monitoring\n",
    "\n",
    "## Acceptance Criteria\n",
    "\n",
    "### ‚â•99% Known-Bad Content Blocked\n",
    "Hash matching successfully catches known harmful content with Hamming distance ‚â§ 30\n",
    "\n",
    "### NSFW False Positive Rate ‚â§1%\n",
    "Adult content classifier maintains high precision on clean sample images\n",
    "\n",
    "### P95 Latency ‚â§250ms\n",
    "Complete moderation decision (hash + NSFW + OCR + rules) within performance target\n",
    "\n",
    "### Teen Protection Effective\n",
    "Age-restricted content properly blocked/hidden from teen users when teen_mode=True\n",
    "\n",
    "### Complete Audit Trail\n",
    "All moderation actions logged with reasons, timestamps, and reviewer decisions for appeals\n",
    "\n",
    "### API Functionality Complete\n",
    "All endpoints operational: /hash, /match, /classify/nsfw, /ocr, /decide with proper error handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}