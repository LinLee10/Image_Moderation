{
 "cells": [
  {
   "cell_type": "code",
   "source": "class ImageModerationSystem:\n    \"\"\"\n    Consolidated image moderation system implementing Phase 1 & 2\n    \n    This is a foundational implementation - components can be used \n    independently or combined based on needs. This is just Phase 1 & 2\n    out of a much larger 7-phase system.\n    \"\"\"\n    \n    def __init__(self, config: Dict[str, Any] = None):\n        self.config = config or {}\n        \n        # Phase 1 Components\n        self.store = HashStore(self.config.get('db_path', 'moderation.db'))\n        self.mih_index = MIHIndex(self.store)\n        self.hma_engine = None  # Initialized when needed\n        self.csam_pipeline = None  # Initialized when needed\n        \n        # Phase 2 Components  \n        self.nsfw_detector = NSFWDetector()\n        self.violence_detector = ViolenceGoreDetector()\n        self.weapon_detector = WeaponDetector()\n        self.hate_symbol_detector = HateSymbolDetector()\n        \n        # System state\n        self.initialized = False\n    \n    def initialize(self):\n        \"\"\"Initialize all system components\"\"\"\n        if self.initialized:\n            return\n            \n        try:\n            # Initialize storage\n            self.store.init()\n            \n            # Build MIH index if data exists\n            if any(True for _ in self.store.iter_all_hashes()):\n                self.mih_index.build()\n            \n            print(\"Image moderation system initialized successfully\")\n            self.initialized = True\n            \n        except Exception as e:\n            logging.error(f\"Error initializing moderation system: {e}\")\n            raise\n    \n    def analyze_image_complete(self, image_path: str, media_id: str = None, \n                              surface_type: str = 'feed') -> Dict[str, Any]:\n        \"\"\"\n        Run complete analysis on an image using all available components\n        This is a basic example - production would have more sophisticated\n        orchestration and decision logic\n        \"\"\"\n        if not self.initialized:\n            self.initialize()\n            \n        if not media_id:\n            media_id = f\"img_{int(time.time())}\"\n        \n        results = {\n            'media_id': media_id,\n            'image_path': image_path,\n            'surface_type': surface_type,\n            'timestamp': datetime.now().isoformat(),\n            'phase1_results': {},\n            'phase2_results': {},\n            'final_action': 'allow',\n            'confidence': 0.0\n        }\n        \n        try:\n            # Phase 1: Hash-based matching\n            hash_hex, quality = PDQHasher.compute_pdq(image_path)\n            matches = self.mih_index.query(hash_hex, max_distance=30)\n            \n            results['phase1_results'] = {\n                'hash': hash_hex,\n                'quality': quality,\n                'matches': matches,\n                'match_count': len(matches)\n            }\n            \n            # If hash matches found, that takes priority\n            if matches:\n                results['final_action'] = 'block'\n                results['confidence'] = 1.0\n                return results\n            \n            # Phase 2: ML-based classification\n            # Note: In production, these might run in parallel\n            \n            # NSFW Detection\n            nsfw_result = self.nsfw_detector.analyze_image(image_path, surface_type)\n            results['phase2_results']['nsfw'] = nsfw_result\n            \n            # Violence/Gore Detection\n            violence_result = self.violence_detector.analyze_image(image_path)\n            results['phase2_results']['violence'] = violence_result\n            \n            # Weapon Detection\n            weapon_result = self.weapon_detector.detect_weapons(image_path)\n            results['phase2_results']['weapons'] = weapon_result\n            \n            # Hate Symbol Detection\n            hate_result = self.hate_symbol_detector.analyze_image(image_path)\n            results['phase2_results']['hate_symbols'] = hate_result\n            \n            # Simple decision logic (production would be much more sophisticated)\n            if (nsfw_result.get('action') == 'block' or \n                violence_result.get('overall_action') == 'block' or\n                hate_result.get('overall_action') == 'block' or\n                weapon_result.get('overall_confidence', 0) > 0.8):\n                \n                results['final_action'] = 'block'\n                results['confidence'] = max(\n                    nsfw_result.get('nsfw_score', 0),\n                    violence_result.get('max_violence_score', 0),\n                    hate_result.get('confidence', 0),\n                    weapon_result.get('overall_confidence', 0)\n                )\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"Error analyzing image {media_id}: {e}\")\n            results['error'] = str(e)\n            results['final_action'] = 'allow'  # Fail open\n            return results\n\n# System usage examples and integration notes\nprint(\"\"\"\nImage Moderation System - Phase 1 & 2 Implementation Complete\n\nThis notebook contains implementations for:\n\nPHASE 1 - Hash Matching \"Seatbelts\":\n- 1A: PDQ Perceptual Hashing with Multi-Index Hashing\n- 1B: ThreatExchange/HMA Integration Framework  \n- 1C: CSAM/NCII Detection Pipelines (PhotoDNA, CSAI Match, StopNCII)\n\nPHASE 2 - Core Classifiers:\n- 2A: NSFW/Nudity Detection (OpenNSFW2 + CLIP ensemble)\n- 2B: Violence/Gore & Weapons Detection\n- 2C: Hate Symbols & Extremist Content Detection\n\nINTEGRATION NOTES:\n- Each component can be used independently\n- Database schema supports all features\n- Fail-safes and queuing built into hash matching\n- Configurable thresholds per surface type\n- Basic orchestration provided in ImageModerationSystem class\n\nNEXT PHASES (not implemented here):\n- Phase 3: OCR & Meme Understanding  \n- Phase 4: Teen Safety & Age Gates\n- Phase 5: Policy Engine & Actions\n- Phase 6: Privacy & Compliance  \n- Phase 7: Scale & Adversarial Testing\n\nThis is foundational work - production deployment would require\nadditional infrastructure, monitoring, and safety measures.\n\"\"\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Integrated Moderation System\n\nComplete image moderation pipeline combining all Phase 1 and Phase 2 components.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class HateSymbolDetector:\n    \"\"\"Detection of hate symbols and extremist logos\"\"\"\n    \n    def __init__(self):\n        self.models_loaded = False\n        self.clip_model = None\n        self.clip_processor = None\n        self.ocr_reader = None\n        self.symbol_database = HateSymbolDatabase()\n        \n        # Confidence thresholds\n        self.thresholds = {\n            'symbol_detection': 0.7,\n            'text_detection': 0.8,\n            'combined_confidence': 0.6  # Lower when both symbol and text agree\n        }\n    \n    def load_models(self):\n        \"\"\"Load hate symbol detection models\"\"\"\n        try:\n            # Load CLIP model\n            if not self.clip_model:\n                self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n                self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n            \n            # Load OCR model\n            if not self.ocr_reader:\n                self.ocr_reader = easyocr.Reader(['en'])\n            \n            self.models_loaded = True\n            print(\"Hate symbol detection models loaded successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Error loading hate symbol detection models: {e}\")\n    \n    def detect_symbols_with_clip(self, image: Image.Image) -> Dict[str, Any]:\n        \"\"\"Detect hate symbols using CLIP\"\"\"\n        if not self.clip_model:\n            return {}\n        \n        try:\n            # Get known hate symbols and extremist logos\n            symbol_prompts = self.symbol_database.get_symbol_prompts()\n            safe_prompts = [\n                \"normal logo\", \"safe symbol\", \"regular image\", \n                \"appropriate content\", \"brand logo\"\n            ]\n            \n            results = {}\n            \n            for category, prompts in symbol_prompts.items():\n                all_prompts = prompts + safe_prompts\n                \n                inputs = self.clip_processor(\n                    text=all_prompts,\n                    images=image,\n                    return_tensors=\"pt\",\n                    padding=True\n                )\n                \n                with torch.no_grad():\n                    outputs = self.clip_model(**inputs)\n                    logits_per_image = outputs.logits_per_image\n                    probs = logits_per_image.softmax(dim=1)\n                    \n                    # Get maximum confidence for hate symbol prompts\n                    symbol_probs = probs[0][:len(prompts)]\n                    max_symbol_prob = symbol_probs.max().item()\n                    best_match_idx = symbol_probs.argmax().item()\n                    \n                    safe_prob = probs[0][len(prompts):].mean().item()\n                    \n                    # Normalize confidence\n                    total_prob = max_symbol_prob + safe_prob\n                    if total_prob > 0:\n                        confidence = max_symbol_prob / total_prob\n                    else:\n                        confidence = 0.0\n                    \n                    if confidence > self.thresholds['symbol_detection']:\n                        results[category] = {\n                            'confidence': confidence,\n                            'matched_symbol': prompts[best_match_idx],\n                            'detection_method': 'clip'\n                        }\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"CLIP symbol detection error: {e}\")\n            return {}\n    \n    def extract_text_with_ocr(self, image_path: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract text from image using OCR\"\"\"\n        if not self.ocr_reader:\n            return []\n        \n        try:\n            results = self.ocr_reader.readtext(image_path)\n            \n            extracted_text = []\n            for (bbox, text, confidence) in results:\n                if confidence > 0.5:  # Filter low confidence OCR results\n                    extracted_text.append({\n                        'text': text.strip(),\n                        'confidence': confidence,\n                        'bbox': bbox\n                    })\n            \n            return extracted_text\n            \n        except Exception as e:\n            logging.error(f\"OCR extraction error: {e}\")\n            return []\n    \n    def analyze_text_for_hate(self, text_results: List[Dict[str, Any]]) -> Dict[str, Any]:\n        \"\"\"Analyze extracted text for hate speech and extremist content\"\"\"\n        hate_keywords = self.symbol_database.get_hate_keywords()\n        \n        detected_hate_text = []\n        max_confidence = 0.0\n        \n        for text_item in text_results:\n            text_lower = text_item['text'].lower()\n            \n            for category, keywords in hate_keywords.items():\n                for keyword in keywords:\n                    if keyword.lower() in text_lower:\n                        confidence = text_item['confidence'] * 0.9  # Slight penalty for text matching\n                        \n                        detected_hate_text.append({\n                            'category': category,\n                            'matched_keyword': keyword,\n                            'full_text': text_item['text'],\n                            'confidence': confidence,\n                            'bbox': text_item['bbox'],\n                            'detection_method': 'ocr'\n                        })\n                        \n                        max_confidence = max(max_confidence, confidence)\n        \n        return {\n            'hate_text_detected': detected_hate_text,\n            'max_confidence': max_confidence,\n            'total_matches': len(detected_hate_text)\n        }\n    \n    def analyze_image(self, image_path: str) -> Dict[str, Any]:\n        \"\"\"Comprehensive hate symbol and extremist content analysis\"\"\"\n        if not self.models_loaded:\n            self.load_models()\n        \n        try:\n            # Load image\n            image_pil = Image.open(image_path).convert('RGB')\n            \n            # Symbol detection with CLIP\n            symbol_results = self.detect_symbols_with_clip(image_pil)\n            \n            # Text extraction and analysis\n            text_results = self.extract_text_with_ocr(image_path)\n            hate_text_analysis = self.analyze_text_for_hate(text_results)\n            \n            # Combined analysis\n            final_result = {\n                'symbol_detections': symbol_results,\n                'text_detections': hate_text_analysis,\n                'all_extracted_text': [item['text'] for item in text_results],\n                'overall_action': 'allow',\n                'confidence': 0.0,\n                'detection_methods': []\n            }\n            \n            # Determine final action based on combined evidence\n            symbol_confidence = max([r['confidence'] for r in symbol_results.values()], default=0.0)\n            text_confidence = hate_text_analysis['max_confidence']\n            \n            # If both symbol and text detection agree, lower the threshold\n            if symbol_results and hate_text_analysis['hate_text_detected']:\n                combined_confidence = (symbol_confidence + text_confidence) / 2\n                threshold = self.thresholds['combined_confidence']\n                final_result['detection_methods'] = ['symbol', 'text']\n            elif symbol_results:\n                combined_confidence = symbol_confidence\n                threshold = self.thresholds['symbol_detection']\n                final_result['detection_methods'] = ['symbol']\n            elif hate_text_analysis['hate_text_detected']:\n                combined_confidence = text_confidence\n                threshold = self.thresholds['text_detection']\n                final_result['detection_methods'] = ['text']\n            else:\n                combined_confidence = 0.0\n                threshold = 1.0\n            \n            final_result['confidence'] = combined_confidence\n            final_result['overall_action'] = 'block' if combined_confidence > threshold else 'allow'\n            \n            return final_result\n            \n        except Exception as e:\n            logging.error(f\"Hate symbol analysis error: {e}\")\n            return {\n                'error': str(e),\n                'overall_action': 'allow',\n                'confidence': 0.0\n            }\n\nclass HateSymbolDatabase:\n    \"\"\"Database of known hate symbols, logos, and keywords\"\"\"\n    \n    def __init__(self):\n        self.symbol_categories = self._load_symbol_database()\n        self.hate_keywords = self._load_hate_keywords()\n    \n    def _load_symbol_database(self) -> Dict[str, List[str]]:\n        \"\"\"Load database of hate symbols and extremist logos\"\"\"\n        # This would normally be loaded from a curated database\n        return {\n            'nazi_symbols': [\n                'swastika', 'nazi symbol', 'third reich emblem',\n                'ss symbol', 'hitler youth symbol', 'iron cross'\n            ],\n            'white_supremacist': [\n                'confederate flag', 'kkk symbol', 'white power symbol',\n                'celtic cross', 'blood and honor', 'fourteen words'\n            ],\n            'extremist_groups': [\n                'isis flag', 'terrorist logo', 'extremist symbol',\n                'militia patch', 'hate group logo', 'radical emblem'\n            ],\n            'gang_symbols': [\n                'gang tag', 'gang symbol', 'criminal organization logo',\n                'cartel symbol', 'prison gang mark'\n            ]\n        }\n    \n    def _load_hate_keywords(self) -> Dict[str, List[str]]:\n        \"\"\"Load database of hate keywords and phrases\"\"\"\n        # Simplified example - production would use comprehensive, reviewed database\n        return {\n            'racial_slurs': [\n                # Would contain actual slurs but omitted here for safety\n                'hate_keyword_1', 'hate_keyword_2'\n            ],\n            'extremist_phrases': [\n                'white power', 'blood and soil', 'race war',\n                'day of the rope', 'turner diaries'\n            ],\n            'terrorist_language': [\n                'allahu akbar', 'death to america', 'jihad',\n                'martyrdom', 'caliphate'\n            ],\n            'holocaust_denial': [\n                'holohoax', 'six million lie', 'gas chamber myth',\n                'holocaust denial', 'revisionist history'\n            ]\n        }\n    \n    def get_symbol_prompts(self) -> Dict[str, List[str]]:\n        \"\"\"Get CLIP prompts for symbol detection\"\"\"\n        return self.symbol_categories\n    \n    def get_hate_keywords(self) -> Dict[str, List[str]]:\n        \"\"\"Get hate keywords for text analysis\"\"\"\n        return self.hate_keywords\n    \n    def add_symbol_category(self, category: str, prompts: List[str]):\n        \"\"\"Add new symbol category (for updates)\"\"\"\n        self.symbol_categories[category] = prompts\n    \n    def add_hate_keywords(self, category: str, keywords: List[str]):\n        \"\"\"Add new hate keywords (for updates)\"\"\"\n        if category in self.hate_keywords:\n            self.hate_keywords[category].extend(keywords)\n        else:\n            self.hate_keywords[category] = keywords\n    \n    def update_from_external_source(self, source_url: str):\n        \"\"\"Update database from external hate symbol database\"\"\"\n        # Placeholder for integration with external databases like ADL's database\n        try:\n            # Would implement API calls to external databases\n            logging.info(f\"Would update symbols from {source_url}\")\n            pass\n        except Exception as e:\n            logging.error(f\"Error updating from external source: {e}\")\n\nprint(\"Hate symbols and extremist content detection system initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 2C: Hate Symbols & Extremist Logos Detection\n\nSymbol + text fusion to detect hateful propaganda and extremist branding.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ViolenceGoreDetector:\n    \"\"\"Detection of violent and graphic content\"\"\"\n    \n    def __init__(self):\n        self.models_loaded = False\n        self.clip_model = None\n        self.clip_processor = None\n        self.color_analyzer = ColorAnalyzer()\n        \n        # Violence categories and thresholds\n        self.violence_categories = {\n            'blood': 0.7,\n            'injury': 0.8,\n            'corpse': 0.9,\n            'weapons': 0.8,\n            'fighting': 0.75\n        }\n    \n    def load_models(self):\n        \"\"\"Load violence detection models\"\"\"\n        try:\n            if not self.clip_model:\n                self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n                self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n            \n            self.models_loaded = True\n            print(\"Violence detection models loaded successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Error loading violence detection models: {e}\")\n    \n    def analyze_violence_with_clip(self, image: Image.Image) -> Dict[str, float]:\n        \"\"\"Analyze violence using CLIP\"\"\"\n        if not self.clip_model:\n            return {}\n        \n        try:\n            violence_prompts = {\n                'blood': [\n                    \"blood\", \"bloody scene\", \"red liquid\", \"bleeding\",\n                    \"bloodstain\", \"gore\", \"graphic blood\"\n                ],\n                'injury': [\n                    \"injury\", \"wound\", \"hurt person\", \"medical emergency\",\n                    \"accident victim\", \"injured body\", \"trauma\"\n                ],\n                'corpse': [\n                    \"dead body\", \"corpse\", \"deceased person\", \"death scene\",\n                    \"morgue\", \"funeral\", \"dead animal\"\n                ],\n                'weapons': [\n                    \"gun\", \"knife\", \"weapon\", \"firearm\", \"blade\",\n                    \"rifle\", \"pistol\", \"sword\", \"machete\"\n                ],\n                'fighting': [\n                    \"fighting\", \"violence\", \"attack\", \"assault\",\n                    \"brawl\", \"combat\", \"aggression\"\n                ]\n            }\n            \n            safe_prompts = [\n                \"safe image\", \"normal photo\", \"peaceful scene\",\n                \"family friendly content\", \"appropriate image\"\n            ]\n            \n            results = {}\n            \n            for category, prompts in violence_prompts.items():\n                all_prompts = prompts + safe_prompts\n                \n                inputs = self.clip_processor(\n                    text=all_prompts,\n                    images=image,\n                    return_tensors=\"pt\",\n                    padding=True\n                )\n                \n                with torch.no_grad():\n                    outputs = self.clip_model(**inputs)\n                    logits_per_image = outputs.logits_per_image\n                    probs = logits_per_image.softmax(dim=1)\n                    \n                    # Average probability for violence prompts vs safe prompts\n                    violence_prob = probs[0][:len(prompts)].mean().item()\n                    safe_prob = probs[0][len(prompts):].mean().item()\n                    \n                    # Normalize\n                    total_prob = violence_prob + safe_prob\n                    if total_prob > 0:\n                        violence_score = violence_prob / total_prob\n                    else:\n                        violence_score = 0.0\n                    \n                    results[category] = violence_score\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"CLIP violence analysis error: {e}\")\n            return {}\n    \n    def analyze_color_patterns(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Analyze color patterns indicative of violence/gore\"\"\"\n        return self.color_analyzer.analyze_for_violence(image)\n    \n    def analyze_image(self, image_path: str) -> Dict[str, Any]:\n        \"\"\"Comprehensive violence/gore analysis\"\"\"\n        if not self.models_loaded:\n            self.load_models()\n        \n        try:\n            # Load image\n            image_pil = Image.open(image_path).convert('RGB')\n            image_cv = cv2.imread(image_path)\n            image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n            \n            # CLIP-based analysis\n            clip_results = self.analyze_violence_with_clip(image_pil)\n            \n            # Color pattern analysis\n            color_results = self.analyze_color_patterns(image_cv_rgb)\n            \n            # Combine results\n            final_scores = {}\n            actions = {}\n            \n            for category in self.violence_categories:\n                clip_score = clip_results.get(category, 0.0)\n                color_score = color_results.get(category, 0.0)\n                \n                # Weighted combination\n                combined_score = (clip_score * 0.7 + color_score * 0.3)\n                final_scores[category] = combined_score\n                \n                # Action decision\n                threshold = self.violence_categories[category]\n                actions[category] = 'block' if combined_score > threshold else 'allow'\n            \n            # Overall action (block if any category triggers)\n            overall_action = 'block' if 'block' in actions.values() else 'allow'\n            max_score = max(final_scores.values()) if final_scores else 0.0\n            \n            return {\n                'violence_scores': final_scores,\n                'clip_results': clip_results,\n                'color_results': color_results,\n                'category_actions': actions,\n                'overall_action': overall_action,\n                'max_violence_score': max_score,\n                'graphicness': self._calculate_graphicness(final_scores)\n            }\n            \n        except Exception as e:\n            logging.error(f\"Violence analysis error: {e}\")\n            return {\n                'error': str(e),\n                'overall_action': 'allow',\n                'violence_scores': {}\n            }\n    \n    def _calculate_graphicness(self, scores: Dict[str, float]) -> float:\n        \"\"\"Calculate overall graphicness score\"\"\"\n        if not scores:\n            return 0.0\n        \n        # Weight different categories for graphicness\n        weights = {\n            'blood': 0.3,\n            'injury': 0.2,\n            'corpse': 0.4,\n            'weapons': 0.05,\n            'fighting': 0.05\n        }\n        \n        weighted_sum = sum(scores.get(cat, 0) * weight \n                          for cat, weight in weights.items())\n        return min(1.0, weighted_sum)\n\nclass WeaponDetector:\n    \"\"\"Specialized weapon detection\"\"\"\n    \n    def __init__(self):\n        self.models_loaded = False\n        self.clip_model = None\n        self.clip_processor = None\n        self.shape_detector = WeaponShapeDetector()\n        \n        self.weapon_types = {\n            'firearm': ['gun', 'pistol', 'rifle', 'shotgun', 'revolver'],\n            'blade': ['knife', 'sword', 'machete', 'dagger', 'blade'],\n            'blunt': ['bat', 'club', 'hammer', 'pipe', 'stick']\n        }\n    \n    def load_models(self):\n        \"\"\"Load weapon detection models\"\"\"\n        try:\n            if not self.clip_model:\n                self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n                self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n            \n            self.models_loaded = True\n            print(\"Weapon detection models loaded successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Error loading weapon detection models: {e}\")\n    \n    def detect_weapons(self, image_path: str) -> Dict[str, Any]:\n        \"\"\"Detect weapons in image\"\"\"\n        if not self.models_loaded:\n            self.load_models()\n        \n        try:\n            image_pil = Image.open(image_path).convert('RGB')\n            image_cv = cv2.imread(image_path)\n            \n            results = {\n                'weapons_detected': [],\n                'confidence_scores': {},\n                'shape_analysis': {},\n                'overall_confidence': 0.0\n            }\n            \n            # CLIP-based weapon detection\n            for weapon_type, weapon_names in self.weapon_types.items():\n                all_prompts = weapon_names + ['safe object', 'household item', 'tool']\n                \n                inputs = self.clip_processor(\n                    text=all_prompts,\n                    images=image_pil,\n                    return_tensors=\"pt\",\n                    padding=True\n                )\n                \n                with torch.no_grad():\n                    outputs = self.clip_model(**inputs)\n                    logits_per_image = outputs.logits_per_image\n                    probs = logits_per_image.softmax(dim=1)\n                    \n                    weapon_prob = probs[0][:len(weapon_names)].max().item()\n                    results['confidence_scores'][weapon_type] = weapon_prob\n                    \n                    if weapon_prob > 0.7:  # High confidence threshold\n                        results['weapons_detected'].append(weapon_type)\n            \n            # Shape-based detection\n            shape_results = self.shape_detector.detect_weapon_shapes(image_cv)\n            results['shape_analysis'] = shape_results\n            \n            # Overall confidence\n            if results['confidence_scores']:\n                results['overall_confidence'] = max(results['confidence_scores'].values())\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"Weapon detection error: {e}\")\n            return {'error': str(e), 'weapons_detected': []}\n\nclass ColorAnalyzer:\n    \"\"\"Color pattern analysis for violence detection\"\"\"\n    \n    def analyze_for_violence(self, image: np.ndarray) -> Dict[str, float]:\n        \"\"\"Analyze color patterns indicative of violence\"\"\"\n        try:\n            hsv_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n            \n            results = {\n                'blood': self._detect_blood_colors(hsv_image),\n                'injury': self._detect_injury_colors(hsv_image),\n                'corpse': self._detect_corpse_colors(hsv_image)\n            }\n            \n            return results\n            \n        except Exception as e:\n            logging.error(f\"Color analysis error: {e}\")\n            return {'blood': 0.0, 'injury': 0.0, 'corpse': 0.0}\n    \n    def _detect_blood_colors(self, hsv_image: np.ndarray) -> float:\n        \"\"\"Detect blood-like red colors\"\"\"\n        # Define HSV ranges for blood-like colors\n        blood_lower = np.array([0, 50, 50])\n        blood_upper = np.array([10, 255, 200])\n        \n        blood_mask = cv2.inRange(hsv_image, blood_lower, blood_upper)\n        blood_pixels = np.sum(blood_mask > 0)\n        total_pixels = hsv_image.shape[0] * hsv_image.shape[1]\n        \n        blood_ratio = blood_pixels / total_pixels\n        return min(1.0, blood_ratio * 10)  # Scale factor\n    \n    def _detect_injury_colors(self, hsv_image: np.ndarray) -> float:\n        \"\"\"Detect colors associated with injuries (bruises, etc.)\"\"\"\n        # Purple/blue ranges for bruising\n        bruise_lower = np.array([120, 30, 30])\n        bruise_upper = np.array([150, 255, 150])\n        \n        bruise_mask = cv2.inRange(hsv_image, bruise_lower, bruise_upper)\n        bruise_pixels = np.sum(bruise_mask > 0)\n        total_pixels = hsv_image.shape[0] * hsv_image.shape[1]\n        \n        bruise_ratio = bruise_pixels / total_pixels\n        return min(1.0, bruise_ratio * 8)\n    \n    def _detect_corpse_colors(self, hsv_image: np.ndarray) -> float:\n        \"\"\"Detect colors associated with death (pale, grey tones)\"\"\"\n        # Grey/pale color ranges\n        pale_lower = np.array([0, 0, 40])\n        pale_upper = np.array([180, 30, 120])\n        \n        pale_mask = cv2.inRange(hsv_image, pale_lower, pale_upper)\n        pale_pixels = np.sum(pale_mask > 0)\n        total_pixels = hsv_image.shape[0] * hsv_image.shape[1]\n        \n        pale_ratio = pale_pixels / total_pixels\n        return min(1.0, pale_ratio * 5)\n\nclass WeaponShapeDetector:\n    \"\"\"Shape-based weapon detection using basic computer vision\"\"\"\n    \n    def detect_weapon_shapes(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Detect weapon-like shapes\"\"\"\n        try:\n            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n            \n            # Find contours\n            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            weapon_shapes = []\n            \n            for contour in contours:\n                if cv2.contourArea(contour) > 1000:  # Filter small shapes\n                    # Analyze shape characteristics\n                    x, y, w, h = cv2.boundingRect(contour)\n                    aspect_ratio = w / h if h > 0 else 0\n                    \n                    # Simple heuristics for weapon-like shapes\n                    if self._is_gun_like_shape(contour, aspect_ratio):\n                        weapon_shapes.append({\n                            'type': 'gun_like',\n                            'bbox': [x, y, w, h],\n                            'confidence': 0.6  # Conservative confidence\n                        })\n                    elif self._is_knife_like_shape(contour, aspect_ratio):\n                        weapon_shapes.append({\n                            'type': 'blade_like',\n                            'bbox': [x, y, w, h],\n                            'confidence': 0.5\n                        })\n            \n            return {\n                'detected_shapes': weapon_shapes,\n                'shape_count': len(weapon_shapes)\n            }\n            \n        except Exception as e:\n            logging.error(f\"Shape detection error: {e}\")\n            return {'detected_shapes': [], 'shape_count': 0}\n    \n    def _is_gun_like_shape(self, contour: np.ndarray, aspect_ratio: float) -> bool:\n        \"\"\"Check if contour resembles a gun shape\"\"\"\n        # Very basic heuristics - would need ML model for production\n        if 2.0 < aspect_ratio < 6.0:  # Elongated shape\n            hull = cv2.convexHull(contour)\n            hull_area = cv2.contourArea(hull)\n            contour_area = cv2.contourArea(contour)\n            \n            if hull_area > 0:\n                solidity = contour_area / hull_area\n                return 0.7 < solidity < 0.95  # Relatively solid but with some concavity\n        \n        return False\n    \n    def _is_knife_like_shape(self, contour: np.ndarray, aspect_ratio: float) -> bool:\n        \"\"\"Check if contour resembles a knife/blade shape\"\"\"\n        if 3.0 < aspect_ratio < 10.0:  # Very elongated\n            # Check for pointed end (simplified)\n            hull = cv2.convexHull(contour)\n            hull_area = cv2.contourArea(hull)\n            contour_area = cv2.contourArea(contour)\n            \n            if hull_area > 0:\n                solidity = contour_area / hull_area\n                return solidity > 0.8  # Should be quite solid for a blade\n        \n        return False\n\nprint(\"Violence/gore and weapons detection system initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 2B: Violence/Gore and Weapons Detection\n\nMulti-label classifier for graphic content and weapon detection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class NSFWDetector:\n    \"\"\"NSFW detection using ensemble of models\"\"\"\n    \n    def __init__(self):\n        self.models_loaded = False\n        self.opennsfw_model = None\n        self.clip_model = None\n        self.clip_processor = None\n        self.region_detector = None\n        \n        # Thresholds per surface type\n        self.thresholds = {\n            'feed': 0.8,\n            'avatar': 0.9,\n            'dm': 0.7\n        }\n    \n    def load_models(self):\n        \"\"\"Load NSFW detection models\"\"\"\n        try:\n            # Load OpenNSFW2 (if available)\n            try:\n                import opennsfw2 as n2\n                self.opennsfw_model = n2\n                print(\"OpenNSFW2 loaded\")\n            except ImportError:\n                print(\"OpenNSFW2 not available, installing...\")\n                os.system(\"pip install opennsfw2\")\n                import opennsfw2 as n2\n                self.opennsfw_model = n2\n            \n            # Load CLIP model for NSFW detection\n            self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n            self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n            \n            # Create simple region detector using basic CV\n            self.region_detector = RegionDetector()\n            \n            self.models_loaded = True\n            print(\"NSFW detection models loaded successfully\")\n            \n        except Exception as e:\n            logging.error(f\"Error loading NSFW models: {e}\")\n    \n    def analyze_with_opennsfw(self, image_path: str) -> float:\n        \"\"\"Analyze image with OpenNSFW2\"\"\"\n        if not self.opennsfw_model:\n            return 0.0\n            \n        try:\n            nsfw_probability = self.opennsfw_model.predict_image(image_path)\n            return float(nsfw_probability)\n        except Exception as e:\n            logging.error(f\"OpenNSFW2 error: {e}\")\n            return 0.0\n    \n    def analyze_with_clip(self, image: Image.Image) -> float:\n        \"\"\"Analyze image with CLIP-based NSFW classifier\"\"\"\n        if not self.clip_model:\n            return 0.0\n            \n        try:\n            # Define NSFW-related text prompts\n            nsfw_prompts = [\n                \"explicit sexual content\",\n                \"nudity\",\n                \"pornographic image\",\n                \"adult content\",\n                \"sexual activity\"\n            ]\n            \n            safe_prompts = [\n                \"safe for work image\",\n                \"normal photo\",\n                \"appropriate content\",\n                \"family friendly image\"\n            ]\n            \n            inputs = self.clip_processor(\n                text=nsfw_prompts + safe_prompts,\n                images=image,\n                return_tensors=\"pt\",\n                padding=True\n            )\n            \n            with torch.no_grad():\n                outputs = self.clip_model(**inputs)\n                logits_per_image = outputs.logits_per_image\n                probs = logits_per_image.softmax(dim=1)\n                \n                # Average NSFW probability\n                nsfw_prob = probs[0][:len(nsfw_prompts)].mean().item()\n                return nsfw_prob\n                \n        except Exception as e:\n            logging.error(f\"CLIP NSFW error: {e}\")\n            return 0.0\n    \n    def detect_regions(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Detect specific body regions\"\"\"\n        if not self.region_detector:\n            return {'regions': [], 'confidence': 0.0}\n            \n        return self.region_detector.detect_regions(image)\n    \n    def analyze_image(self, image_path: str, surface_type: str = 'feed') -> Dict[str, Any]:\n        \"\"\"Comprehensive NSFW analysis\"\"\"\n        if not self.models_loaded:\n            self.load_models()\n        \n        try:\n            # Load image\n            image_pil = Image.open(image_path).convert('RGB')\n            image_cv = cv2.imread(image_path)\n            image_cv_rgb = cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)\n            \n            # OpenNSFW2 analysis\n            opennsfw_score = self.analyze_with_opennsfw(image_path)\n            \n            # CLIP analysis\n            clip_score = self.analyze_with_clip(image_pil)\n            \n            # Region detection\n            regions_result = self.detect_regions(image_cv_rgb)\n            \n            # Ensemble scoring\n            ensemble_score = (opennsfw_score * 0.6 + clip_score * 0.4)\n            \n            # Apply region weighting\n            region_penalty = regions_result['confidence'] * 0.2\n            final_score = min(1.0, ensemble_score + region_penalty)\n            \n            # Threshold decision\n            threshold = self.thresholds.get(surface_type, 0.8)\n            action = 'block' if final_score > threshold else 'allow'\n            \n            return {\n                'nsfw_score': final_score,\n                'opennsfw_score': opennsfw_score,\n                'clip_score': clip_score,\n                'regions': regions_result['regions'],\n                'region_confidence': regions_result['confidence'],\n                'threshold': threshold,\n                'action': action,\n                'surface_type': surface_type\n            }\n            \n        except Exception as e:\n            logging.error(f\"NSFW analysis error: {e}\")\n            return {\n                'error': str(e),\n                'action': 'allow',  # Fail open for safety\n                'nsfw_score': 0.0\n            }\n\nclass RegionDetector:\n    \"\"\"Simple region detector for body parts\"\"\"\n    \n    def __init__(self):\n        # Load basic classifiers for skin detection\n        self.skin_detector = self._create_skin_detector()\n    \n    def _create_skin_detector(self):\n        \"\"\"Create simple skin color detector\"\"\"\n        # HSV ranges for skin color detection\n        return {\n            'lower': np.array([0, 20, 70], dtype=np.uint8),\n            'upper': np.array([20, 255, 255], dtype=np.uint8)\n        }\n    \n    def detect_skin_regions(self, image: np.ndarray) -> np.ndarray:\n        \"\"\"Detect skin-colored regions\"\"\"\n        hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n        \n        skin_mask = cv2.inRange(\n            hsv,\n            self.skin_detector['lower'],\n            self.skin_detector['upper']\n        )\n        \n        # Morphological operations to clean up mask\n        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n        skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_OPEN, kernel)\n        skin_mask = cv2.morphologyEx(skin_mask, cv2.MORPH_CLOSE, kernel)\n        \n        return skin_mask\n    \n    def detect_regions(self, image: np.ndarray) -> Dict[str, Any]:\n        \"\"\"Detect and classify body regions\"\"\"\n        try:\n            skin_mask = self.detect_skin_regions(image)\n            \n            # Find contours\n            contours, _ = cv2.findContours(\n                skin_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n            )\n            \n            regions = []\n            total_skin_area = 0\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 1000:  # Filter small regions\n                    x, y, w, h = cv2.boundingRect(contour)\n                    \n                    # Simple heuristics for region classification\n                    aspect_ratio = w / h if h > 0 else 0\n                    relative_area = area / (image.shape[0] * image.shape[1])\n                    \n                    region_type = self._classify_region(\n                        x, y, w, h, aspect_ratio, relative_area, image.shape\n                    )\n                    \n                    regions.append({\n                        'type': region_type,\n                        'bbox': [x, y, w, h],\n                        'area': area,\n                        'confidence': min(1.0, area / 10000)\n                    })\n                    \n                    total_skin_area += area\n            \n            # Overall confidence based on skin area ratio\n            skin_ratio = total_skin_area / (image.shape[0] * image.shape[1])\n            confidence = min(1.0, skin_ratio * 3)  # Scale factor\n            \n            return {\n                'regions': regions,\n                'confidence': confidence,\n                'total_skin_area': total_skin_area,\n                'skin_ratio': skin_ratio\n            }\n            \n        except Exception as e:\n            logging.error(f\"Region detection error: {e}\")\n            return {'regions': [], 'confidence': 0.0}\n    \n    def _classify_region(self, x: int, y: int, w: int, h: int, \n                        aspect_ratio: float, relative_area: float, \n                        image_shape: Tuple[int, int, int]) -> str:\n        \"\"\"Simple heuristic region classification\"\"\"\n        img_h, img_w = image_shape[:2]\n        \n        # Normalize coordinates\n        center_x = (x + w/2) / img_w\n        center_y = (y + h/2) / img_h\n        \n        # Very simple rules - would need proper ML model for production\n        if center_y < 0.3 and relative_area > 0.05:\n            return 'torso'\n        elif center_y < 0.7 and relative_area > 0.02:\n            return 'limb'\n        elif relative_area > 0.1:\n            return 'large_skin_area'\n        else:\n            return 'skin_region'\n\nprint(\"NSFW detection system initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 2A: Nudity/Sexual Content Detector\n\nEnsemble model to distinguish explicit sexual content with fine-grained region detection.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class PhotoDNAIntegration:\n    \"\"\"Integration with Microsoft PhotoDNA for CSAM detection\"\"\"\n    \n    def __init__(self, api_key: str, endpoint: str):\n        self.api_key = api_key\n        self.endpoint = endpoint\n        self.headers = {\n            'Ocp-Apim-Subscription-Key': api_key,\n            'Content-Type': 'application/octet-stream'\n        }\n    \n    def analyze_image(self, image_bytes: bytes) -> Dict[str, Any]:\n        \"\"\"Analyze image using PhotoDNA\"\"\"\n        try:\n            response = requests.post(\n                f\"{self.endpoint}/photodna/v1.0/Match\",\n                headers=self.headers,\n                data=image_bytes\n            )\n            response.raise_for_status()\n            return response.json()\n        except requests.RequestException as e:\n            logging.error(f\"PhotoDNA API error: {e}\")\n            return {\"error\": str(e)}\n\nclass CSAIMatchIntegration:\n    \"\"\"Integration with Google CSAI Match\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.base_url = \"https://safesearch.googleapis.com/v1\"\n    \n    def analyze_image(self, image_bytes: bytes) -> Dict[str, Any]:\n        \"\"\"Analyze image using CSAI Match\"\"\"\n        try:\n            import base64\n            image_b64 = base64.b64encode(image_bytes).decode('utf-8')\n            \n            payload = {\n                'image': {\n                    'content': image_b64\n                },\n                'features': [\n                    {'type': 'SAFE_SEARCH_DETECTION'},\n                    {'type': 'OBJECT_LOCALIZATION'}\n                ]\n            }\n            \n            response = requests.post(\n                f\"{self.base_url}/images:annotate?key={self.api_key}\",\n                json=payload\n            )\n            response.raise_for_status()\n            return response.json()\n        except Exception as e:\n            logging.error(f\"CSAI Match API error: {e}\")\n            return {\"error\": str(e)}\n\nclass StopNCIIIntegration:\n    \"\"\"StopNCII client-side hashing integration\"\"\"\n    \n    def __init__(self, store: HashStore):\n        self.store = store\n        self.stopncii_hashes = set()\n    \n    def load_survivor_hashes(self, hash_file: str):\n        \"\"\"Load hashes submitted by survivors via StopNCII\"\"\"\n        try:\n            with open(hash_file, 'r') as f:\n                for line in f:\n                    hash_hex = line.strip()\n                    if len(hash_hex) == 64:  # Valid PDQ hash\n                        self.stopncii_hashes.add(hash_hex)\n                        \n                        media_id = f\"stopncii_{hashlib.md5(hash_hex.encode()).hexdigest()[:16]}\"\n                        self.store.upsert_media_hash(\n                            media_id=media_id,\n                            hash_hex=hash_hex,\n                            quality=100,\n                            source=\"stopncii:survivor_submitted\"\n                        )\n            \n            logging.info(f\"Loaded {len(self.stopncii_hashes)} StopNCII hashes\")\n        except Exception as e:\n            logging.error(f\"Error loading StopNCII hashes: {e}\")\n    \n    def check_against_survivor_hashes(self, hash_hex: str, threshold: int = 5) -> bool:\n        \"\"\"Check if image hash matches survivor-submitted content\"\"\"\n        for survivor_hash in self.stopncii_hashes:\n            if HammingUtils.hamming_distance_hex(hash_hex, survivor_hash) <= threshold:\n                return True\n        return False\n\nclass CSAMReportingPipeline:\n    \"\"\"Mandatory reporting pipeline for CSAM/NCII\"\"\"\n    \n    def __init__(self):\n        self.reports_pending = Queue()\n        self.evidence_store = {}\n    \n    def create_ncmec_report(self, media_id: str, hash_hex: str, \n                           detection_source: str, confidence: float) -> Dict[str, Any]:\n        \"\"\"Create NCMEC CyberTipline report structure\"\"\"\n        report = {\n            'report_id': f\"ncmec_{int(time.time())}_{media_id}\",\n            'timestamp': datetime.now().isoformat(),\n            'media_id': media_id,\n            'hash': hash_hex,\n            'detection_method': detection_source,\n            'confidence': confidence,\n            'reporter_info': {\n                'company': 'Content Moderation System',\n                'contact': 'moderation@company.com'\n            },\n            'status': 'pending_review'\n        }\n        \n        self.reports_pending.put(report)\n        self.evidence_store[report['report_id']] = {\n            'hash': hash_hex,\n            'media_id': media_id,\n            'access_log': [],\n            'retention_until': datetime.now().timestamp() + (365 * 24 * 3600)  # 1 year\n        }\n        \n        return report\n    \n    def create_iwf_report(self, media_id: str, hash_hex: str, \n                         detection_source: str) -> Dict[str, Any]:\n        \"\"\"Create IWF (Internet Watch Foundation) report structure\"\"\"\n        report = {\n            'report_id': f\"iwf_{int(time.time())}_{media_id}\",\n            'timestamp': datetime.now().isoformat(),\n            'media_id': media_id,\n            'hash': hash_hex,\n            'detection_method': detection_source,\n            'reporter_info': {\n                'organization': 'Content Moderation System',\n                'country': 'US'\n            },\n            'status': 'pending_submission'\n        }\n        \n        return report\n    \n    def log_evidence_access(self, report_id: str, accessor: str, purpose: str):\n        \"\"\"Log access to evidence with least privilege principle\"\"\"\n        if report_id in self.evidence_store:\n            self.evidence_store[report_id]['access_log'].append({\n                'timestamp': datetime.now().isoformat(),\n                'accessor': accessor,\n                'purpose': purpose\n            })\n\nclass SpecializedCSAMPipeline:\n    \"\"\"Integrated CSAM/NCII detection and reporting pipeline\"\"\"\n    \n    def __init__(self, store: HashStore, photodna_key: str = None, \n                 photodna_endpoint: str = None, csai_key: str = None):\n        self.store = store\n        self.reporting = CSAMReportingPipeline()\n        self.stopncii = StopNCIIIntegration(store)\n        \n        self.photodna = None\n        if photodna_key and photodna_endpoint:\n            self.photodna = PhotoDNAIntegration(photodna_key, photodna_endpoint)\n        \n        self.csai_match = None\n        if csai_key:\n            self.csai_match = CSAIMatchIntegration(csai_key)\n    \n    def analyze_image(self, media_id: str, image_bytes: bytes, \n                     hash_hex: str) -> Dict[str, Any]:\n        \"\"\"Comprehensive CSAM/NCII analysis\"\"\"\n        results = {\n            'media_id': media_id,\n            'hash': hash_hex,\n            'detections': [],\n            'action': 'allow',\n            'confidence': 0.0,\n            'reports_created': []\n        }\n        \n        # Check against StopNCII survivor hashes\n        if self.stopncii.check_against_survivor_hashes(hash_hex):\n            results['detections'].append({\n                'source': 'stopncii',\n                'type': 'ncii_match',\n                'confidence': 1.0\n            })\n            results['action'] = 'block'\n            results['confidence'] = 1.0\n        \n        # PhotoDNA analysis\n        if self.photodna:\n            photodna_result = self.photodna.analyze_image(image_bytes)\n            if 'IsMatch' in photodna_result and photodna_result['IsMatch']:\n                results['detections'].append({\n                    'source': 'photodna',\n                    'type': 'csam_match',\n                    'confidence': photodna_result.get('MatchConfidence', 1.0)\n                })\n                results['action'] = 'block'\n                results['confidence'] = max(results['confidence'], \n                                          photodna_result.get('MatchConfidence', 1.0))\n        \n        # CSAI Match analysis\n        if self.csai_match:\n            csai_result = self.csai_match.analyze_image(image_bytes)\n            if 'responses' in csai_result:\n                for response in csai_result['responses']:\n                    safe_search = response.get('safeSearchAnnotation', {})\n                    if safe_search.get('adult') == 'VERY_LIKELY':\n                        results['detections'].append({\n                            'source': 'csai_match',\n                            'type': 'adult_content',\n                            'confidence': 0.9\n                        })\n                        results['confidence'] = max(results['confidence'], 0.9)\n        \n        # Create mandatory reports if CSAM detected\n        if results['confidence'] > 0.8:\n            ncmec_report = self.reporting.create_ncmec_report(\n                media_id, hash_hex, \n                ';'.join([d['source'] for d in results['detections']]),\n                results['confidence']\n            )\n            results['reports_created'].append(ncmec_report['report_id'])\n            \n            iwf_report = self.reporting.create_iwf_report(\n                media_id, hash_hex,\n                ';'.join([d['source'] for d in results['detections']])\n            )\n            results['reports_created'].append(iwf_report['report_id'])\n        \n        return results\n\nprint(\"CSAM/NCII specialized pipelines initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 1C: CSAM/NCII Specialized Pipelines\n\nIntegration with PhotoDNA, CSAI Match, and StopNCII for detecting abusive material.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class ThreatExchangeIntegration:\n    \"\"\"Integration with Meta's ThreatExchange for hash sharing\"\"\"\n    \n    def __init__(self, access_token: str, store: HashStore):\n        self.access_token = access_token\n        self.store = store\n        self.base_url = \"https://graph.facebook.com/v18.0\"\n        \n    def fetch_threat_indicators(self, privacy_type: str = \"HAS_PRIVACY_GROUP\") -> List[Dict]:\n        \"\"\"Fetch threat indicators from ThreatExchange\"\"\"\n        url = f\"{self.base_url}/threat_indicators\"\n        params = {\n            'access_token': self.access_token,\n            'limit': 1000,\n            'fields': 'indicator,type,privacy_type,tags,status',\n            'privacy_type': privacy_type\n        }\n        \n        indicators = []\n        try:\n            response = requests.get(url, params=params)\n            response.raise_for_status()\n            \n            data = response.json()\n            indicators.extend(data.get('data', []))\n            \n            while data.get('paging', {}).get('next'):\n                response = requests.get(data['paging']['next'])\n                response.raise_for_status()\n                data = response.json()\n                indicators.extend(data.get('data', []))\n                \n        except requests.RequestException as e:\n            logging.error(f\"Error fetching ThreatExchange indicators: {e}\")\n            \n        return indicators\n    \n    def sync_pdq_hashes(self):\n        \"\"\"Sync PDQ hashes from ThreatExchange\"\"\"\n        indicators = self.fetch_threat_indicators()\n        synced_count = 0\n        \n        for indicator in indicators:\n            if indicator.get('type') == 'HASH_PDQ' and indicator.get('status') == 'MALICIOUS':\n                pdq_hash = indicator.get('indicator', '')\n                if len(pdq_hash) == 64:  # Valid PDQ hash length\n                    media_id = f\"threatexchange_{indicator.get('id', pdq_hash[:16])}\"\n                    tags = indicator.get('tags', {}).get('data', [])\n                    tag_names = [tag.get('text', '') for tag in tags]\n                    \n                    self.store.upsert_media_hash(\n                        media_id=media_id,\n                        hash_hex=pdq_hash,\n                        quality=100,  # Assume high quality for curated hashes\n                        source=f\"threatexchange:{','.join(tag_names)}\"\n                    )\n                    synced_count += 1\n        \n        logging.info(f\"Synced {synced_count} PDQ hashes from ThreatExchange\")\n        return synced_count\n\nclass HMAActionEngine:\n    \"\"\"Hasher-Matcher-Actioner pattern implementation\"\"\"\n    \n    def __init__(self, store: HashStore, mih_index: MIHIndex):\n        self.store = store\n        self.mih_index = mih_index\n        self.action_rules = {}\n        self.audit_log = []\n    \n    def add_action_rule(self, rule_name: str, source_pattern: str, \n                       max_distance: int, action: str, priority: int = 100):\n        \"\"\"Add match  action rule\"\"\"\n        self.action_rules[rule_name] = {\n            'source_pattern': source_pattern,\n            'max_distance': max_distance,\n            'action': action,  # 'block', 'blur', 'queue', 'flag'\n            'priority': priority\n        }\n    \n    def process_image(self, media_id: str, image_path: str = None, \n                     image_bytes: bytes = None) -> Dict[str, Any]:\n        \"\"\"Process image through hash-match-action pipeline\"\"\"\n        try:\n            # Hash\n            if image_path:\n                hash_hex, quality = PDQHasher.compute_pdq(image_path)\n            elif image_bytes:\n                hash_hex, quality = PDQHasher.compute_pdq_from_bytes(image_bytes)\n            else:\n                raise ValueError(\"Must provide either image_path or image_bytes\")\n            \n            # Match\n            matches = []\n            for rule_name, rule in self.action_rules.items():\n                rule_matches = self.mih_index.query(hash_hex, rule['max_distance'])\n                \n                for match_id, distance in rule_matches:\n                    try:\n                        _, _, _, source = self.store.get_media_info(match_id)\n                        if rule['source_pattern'] in source:\n                            matches.append({\n                                'rule': rule_name,\n                                'match_id': match_id,\n                                'distance': distance,\n                                'action': rule['action'],\n                                'priority': rule['priority'],\n                                'source': source\n                            })\n                    except:\n                        continue\n            \n            # Act (highest priority action wins)\n            final_action = 'allow'\n            if matches:\n                matches.sort(key=lambda x: x['priority'])\n                final_action = matches[0]['action']\n            \n            # Audit log\n            self.audit_log.append({\n                'timestamp': datetime.now().isoformat(),\n                'media_id': media_id,\n                'hash': hash_hex,\n                'quality': quality,\n                'matches': matches,\n                'final_action': final_action\n            })\n            \n            return {\n                'media_id': media_id,\n                'hash': hash_hex,\n                'quality': quality,\n                'matches': matches,\n                'action': final_action\n            }\n            \n        except Exception as e:\n            logging.error(f\"Error in HMA pipeline for {media_id}: {e}\")\n            return {\n                'media_id': media_id,\n                'error': str(e),\n                'action': 'error'\n            }\n\n# Extend HashStore for HMA integration\ndef get_media_info(self, media_id: str) -> Tuple[str, int, str, str]:\n    \"\"\"Get full media info including source\"\"\"\n    with sqlite3.connect(self.db_path) as conn:\n        cursor = conn.execute('''\n            SELECT hash_hex, quality, created_at, source FROM media_hashes \n            WHERE media_id = ?\n        ''', (media_id,))\n        row = cursor.fetchone()\n        if row:\n            return row[0], row[1], row[2], row[3]\n        raise KeyError(f\"Media ID not found: {media_id}\")\n\nHashStore.get_media_info = get_media_info\n\nprint(\"ThreatExchange and HMA integration initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 1B: Industry Hash Sources + HMA Integration\n\nConnect to shared hash banks (GIFCT, ThreatExchange) to blockade bad content and keep lists fresh.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class MIHIndex:\n    \"\"\"Multi-Index Hashing for fast approximate search\"\"\"\n    \n    def __init__(self, store: HashStore, m: int = 16):\n        self.store = store\n        self.m = m  # Number of chunks\n        self.bits_per_chunk = 256 // m\n        self.index_healthy = True\n    \n    def subkeys_from_hex(self, hash_hex: str) -> List[int]:\n        \"\"\"Extract subkeys from hash for MIH indexing\"\"\"\n        hash_int = HammingUtils.hex_to_int(hash_hex)\n        subkeys = []\n        \n        for chunk in range(self.m):\n            shift = (self.m - 1 - chunk) * self.bits_per_chunk\n            subkey = (hash_int >> shift) & ((1 << self.bits_per_chunk) - 1)\n            subkeys.append(subkey)\n        \n        return subkeys\n    \n    def build(self):\n        \"\"\"Build MIH index from all stored hashes\"\"\"\n        try:\n            self.store.clear_mih()\n            \n            for media_id, hash_hex, quality in self.store.iter_all_hashes():\n                subkeys = self.subkeys_from_hex(hash_hex)\n                \n                for chunk, subkey in enumerate(subkeys):\n                    self.store.insert_mih_row(chunk, subkey, media_id)\n            \n            self.index_healthy = True\n            logging.info(f\"MIH index built successfully with {self.m} chunks\")\n            \n        except Exception as e:\n            logging.error(f\"Error building MIH index: {e}\")\n            self.index_healthy = False\n    \n    def candidates(self, hash_hex: str) -> Set[str]:\n        \"\"\"Get candidate matches from MIH index\"\"\"\n        if not self.index_healthy:\n            return set()\n        \n        try:\n            return self.store.iter_candidates_for(hash_hex, self.m, self.bits_per_chunk)\n        except Exception as e:\n            logging.error(f\"Error querying MIH index: {e}\")\n            self.index_healthy = False\n            return set()\n    \n    def query(self, hash_hex: str, max_distance: int) -> List[Tuple[str, int]]:\n        \"\"\"Query for similar hashes within max_distance\"\"\"\n        candidates = self.candidates(hash_hex)\n        results = []\n        \n        for media_id in candidates:\n            try:\n                candidate_hash, _ = self.store.get_hash(media_id)\n                distance = HammingUtils.hamming_distance_hex(hash_hex, candidate_hash)\n                \n                if distance <= max_distance:\n                    results.append((media_id, distance))\n            except Exception as e:\n                logging.warning(f\"Error processing candidate {media_id}: {e}\")\n        \n        results.sort(key=lambda x: (x[1], x[0]))\n        return results\n    \n    def add_hash_with_fallback(self, media_id: str, hash_hex: str, quality: int, source: str, \n                              image_path: str = None, image_bytes: bytes = None):\n        \"\"\"Add hash with queue fallback if index is down\"\"\"\n        try:\n            self.store.upsert_media_hash(media_id, hash_hex, quality, source)\n            \n            if self.index_healthy:\n                subkeys = self.subkeys_from_hex(hash_hex)\n                for chunk, subkey in enumerate(subkeys):\n                    self.store.insert_mih_row(chunk, subkey, media_id)\n            else:\n                self.store.queue_image(media_id, image_path, image_bytes)\n                \n        except Exception as e:\n            logging.error(f\"Error adding hash for {media_id}: {e}\")\n            self.store.queue_image(media_id, image_path, image_bytes)\n\nprint(\"MIHIndex initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class HashStore:\n    \"\"\"SQLite-based storage for hashes and Multi-Index Hashing\"\"\"\n    \n    def __init__(self, db_path: str = \"moderation.db\"):\n        self.db_path = db_path\n        os.makedirs(os.path.dirname(db_path) if os.path.dirname(db_path) else \".\", exist_ok=True)\n        \n    def init(self):\n        \"\"\"Initialize database tables\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('''\n                CREATE TABLE IF NOT EXISTS media_hashes (\n                    media_id TEXT PRIMARY KEY,\n                    hash_hex TEXT NOT NULL,\n                    quality INTEGER NOT NULL,\n                    source TEXT NOT NULL,\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            ''')\n            conn.execute('''\n                CREATE TABLE IF NOT EXISTS mih_index (\n                    chunk INTEGER NOT NULL,\n                    subkey INTEGER NOT NULL,\n                    media_id TEXT NOT NULL,\n                    INDEX(chunk, subkey)\n                )\n            ''')\n            conn.execute('''\n                CREATE TABLE IF NOT EXISTS hash_queue (\n                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n                    media_id TEXT NOT NULL,\n                    image_path TEXT,\n                    image_bytes BLOB,\n                    status TEXT DEFAULT 'pending',\n                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n                )\n            ''')\n            conn.commit()\n    \n    def upsert_media_hash(self, media_id: str, hash_hex: str, quality: int, source: str):\n        \"\"\"Insert or update media hash\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('''\n                INSERT OR REPLACE INTO media_hashes \n                (media_id, hash_hex, quality, source) \n                VALUES (?, ?, ?, ?)\n            ''', (media_id, hash_hex, quality, source))\n            conn.commit()\n    \n    def iter_all_hashes(self) -> Iterator[Tuple[str, str, int]]:\n        \"\"\"Iterate over all stored hashes\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute('SELECT media_id, hash_hex, quality FROM media_hashes')\n            for row in cursor:\n                yield row\n    \n    def clear_mih(self):\n        \"\"\"Clear Multi-Index Hash table\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('DELETE FROM mih_index')\n            conn.commit()\n    \n    def insert_mih_row(self, chunk: int, subkey: int, media_id: str):\n        \"\"\"Insert MIH index entry\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('''\n                INSERT INTO mih_index (chunk, subkey, media_id) \n                VALUES (?, ?, ?)\n            ''', (chunk, subkey, media_id))\n            conn.commit()\n    \n    def iter_candidates_for(self, hash_hex: str, m: int, bits_per_chunk: int) -> Set[str]:\n        \"\"\"Find candidate matches using MIH index\"\"\"\n        hash_int = HammingUtils.hex_to_int(hash_hex)\n        candidates = set()\n        \n        with sqlite3.connect(self.db_path) as conn:\n            for chunk in range(m):\n                shift = (m - 1 - chunk) * bits_per_chunk\n                subkey = (hash_int >> shift) & ((1 << bits_per_chunk) - 1)\n                \n                cursor = conn.execute('''\n                    SELECT media_id FROM mih_index \n                    WHERE chunk = ? AND subkey = ?\n                ''', (chunk, subkey))\n                \n                for row in cursor:\n                    candidates.add(row[0])\n        \n        return candidates\n    \n    def get_hash(self, media_id: str) -> Tuple[str, int]:\n        \"\"\"Get hash and quality for media ID\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute('''\n                SELECT hash_hex, quality FROM media_hashes \n                WHERE media_id = ?\n            ''', (media_id,))\n            row = cursor.fetchone()\n            if row:\n                return row[0], row[1]\n            raise KeyError(f\"Media ID not found: {media_id}\")\n    \n    def queue_image(self, media_id: str, image_path: str = None, image_bytes: bytes = None):\n        \"\"\"Queue image for processing when index is down\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            conn.execute('''\n                INSERT INTO hash_queue (media_id, image_path, image_bytes) \n                VALUES (?, ?, ?)\n            ''', (media_id, image_path, image_bytes))\n            conn.commit()\n    \n    def process_queue(self):\n        \"\"\"Process queued images\"\"\"\n        with sqlite3.connect(self.db_path) as conn:\n            cursor = conn.execute('''\n                SELECT id, media_id, image_path, image_bytes FROM hash_queue \n                WHERE status = 'pending'\n            ''')\n            \n            for queue_id, media_id, image_path, image_bytes in cursor:\n                try:\n                    if image_path:\n                        hash_hex, quality = PDQHasher.compute_pdq(image_path)\n                    elif image_bytes:\n                        hash_hex, quality = PDQHasher.compute_pdq_from_bytes(image_bytes)\n                    else:\n                        continue\n                    \n                    self.upsert_media_hash(media_id, hash_hex, quality, \"queued\")\n                    \n                    conn.execute('''\n                        UPDATE hash_queue SET status = 'processed' \n                        WHERE id = ?\n                    ''', (queue_id,))\n                    \n                except Exception as e:\n                    logging.error(f\"Error processing queued image {media_id}: {e}\")\n                    conn.execute('''\n                        UPDATE hash_queue SET status = 'error' \n                        WHERE id = ?\n                    ''', (queue_id,))\n            \n            conn.commit()\n\nprint(\"HashStore initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class PDQHasher:\n    \"\"\"Perceptual hashing using PDQ algorithm\"\"\"\n    \n    @staticmethod\n    def compute_pdq(image_path: str) -> Tuple[str, int]:\n        \"\"\"Compute PDQ hash from image file\"\"\"\n        image = cv2.imread(image_path)\n        if image is None:\n            raise ValueError(f\"Could not load image: {image_path}\")\n        \n        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        hash_vector, quality = pdqhash.compute(image_rgb)\n        \n        hash_int = 0\n        for i, bit in enumerate(hash_vector):\n            if bit:\n                hash_int |= (1 << (255 - i))\n        \n        hash_hex = f\"{hash_int:064x}\"\n        return hash_hex, quality\n    \n    @staticmethod\n    def compute_pdq_from_bytes(image_bytes: bytes) -> Tuple[str, int]:\n        \"\"\"Compute PDQ hash from image bytes\"\"\"\n        with tempfile.NamedTemporaryFile(suffix='.jpg') as tmp_file:\n            tmp_file.write(image_bytes)\n            tmp_file.flush()\n            return PDQHasher.compute_pdq(tmp_file.name)\n\nclass HammingUtils:\n    \"\"\"Hamming distance utilities\"\"\"\n    \n    @staticmethod\n    def hex_to_int(h: str) -> int:\n        return int(h, 16)\n    \n    @staticmethod\n    def hamming_distance_hex(h1: str, h2: str) -> int:\n        int1 = HammingUtils.hex_to_int(h1)\n        int2 = HammingUtils.hex_to_int(h2)\n        return (int1 ^ int2).bit_count()\n\n# Test PDQ hashing\nprint(\"PDQ Hasher initialized successfully\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Phase 1A: PDQ Perceptual Hashing\n\nPDQ turns each image into a 256-bit fingerprint for instant matching against known bad content, even after crops/edits.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import os\nimport sys\nimport json\nimport sqlite3\nimport tempfile\nimport hashlib\nimport requests\nimport numpy as np\nimport cv2\nfrom PIL import Image\nfrom typing import Iterator, Tuple, List, Set, Optional, Dict, Any\nfrom pathlib import Path\nimport logging\nfrom datetime import datetime\nimport threading\nfrom queue import Queue\nimport time\n\ntry:\n    import pdqhash\nexcept ImportError:\n    print(\"Installing pdqhash...\")\n    os.system(\"pip install pdqhash\")\n    import pdqhash\n\ntry:\n    import threatexchange\nexcept ImportError:\n    print(\"Installing threatexchange...\")\n    os.system(\"pip install threatexchange\")\n    import threatexchange\n\ntry:\n    import torch\n    import torchvision.transforms as transforms\n    from torchvision import models\nexcept ImportError:\n    print(\"Installing torch and torchvision...\")\n    os.system(\"pip install torch torchvision\")\n    import torch\n    import torchvision.transforms as transforms\n    from torchvision import models\n\ntry:\n    import transformers\n    from transformers import CLIPProcessor, CLIPModel\nexcept ImportError:\n    print(\"Installing transformers...\")\n    os.system(\"pip install transformers\")\n    import transformers\n    from transformers import CLIPProcessor, CLIPModel\n\ntry:\n    import easyocr\nexcept ImportError:\n    print(\"Installing easyocr...\")\n    os.system(\"pip install easyocr\")\n    import easyocr\n\nlogging.basicConfig(level=logging.INFO)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Image Moderation System - Phase 1 & 2 Implementation\n\nThis notebook implements a comprehensive image moderation system with:\n\n## Phase 1  Hash matching \"seatbelts\"\n- **1A)** Perceptual hashing for images (PDQ)\n- **1B)** Industry hash sources + HMA integration\n- **1C)** CSAM/NCII specialized pipelines\n\n## Phase 2  Core classifiers\n- **2A)** Nudity/sexual content detector\n- **2B)** Violence/gore and weapons detection  \n- **2C)** Hate symbols & extremist logos detection",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}